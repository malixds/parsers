import asyncio
import json
import logging
import os
import re
import time
import uuid
from typing import Any
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent

from schema import DbDTO, AgentData

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CompassParser:
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è compass.com —á–µ—Ä–µ–∑ API
    1. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ API (POST –∑–∞–ø—Ä–æ—Å—ã)
    2. –ü–æ–ª—É—á–µ–Ω–∏–µ HTML –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ª–∏—Å—Ç–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ requests
    3. –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –∏–∑ API –∏ HTML –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
    """
    
    def __init__(
        self,
        save_html_every: int = 20,
        html_save_dir: str = "htmls",
    ) -> None:
        self.source_name = "compass"
        self.base_url = "https://www.compass.com"
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML
        self.save_html_every = save_html_every
        self.html_save_dir = html_save_dir
        self.html_counter = 0
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        if not os.path.exists(self.html_save_dir):
            os.makedirs(self.html_save_dir)
            logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML: {self.html_save_dir}")

    # ---------------------- –≠–¢–ê–ü 1: –ò–ù–î–ï–ö–°–ê–¶–ò–Ø ----------------------

    def get_listings_from_api(self, location: str = "new-york", max_results: int = 1000) -> list[dict]:
        """
        –≠–¢–ê–ü 1 (API): –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ API compass.com
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–≤–∫–ª—é—á–∞—è listing –æ–±—ä–µ–∫—Ç—ã)
        """
        logger.info(f"[1-API] –ü–æ–ª—É—á–∞—é –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ API –¥–ª—è –ª–æ–∫–∞—Ü–∏–∏: {location}")
        
        listings_data = []
        search_result_id = str(uuid.uuid4())
        
        # –ë–∞–∑–æ–≤—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è New York (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –¥–ª—è –¥—Ä—É–≥–∏—Ö –ª–æ–∫–∞—Ü–∏–π)
        # –≠—Ç–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ø–æ–∫—Ä—ã–≤–∞—é—Ç –≤–µ—Å—å —à—Ç–∞—Ç NY
        ne_point = {"latitude": 45.3525295, "longitude": -72.3285732}
        sw_point = {"latitude": 39.9017281, "longitude": -79.2115078}
        viewport_ne = {"lat": 45.2954092, "lng": -72.3285732}
        viewport_sw = {"lat": 39.839376, "lng": -79.2115078}
        
        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å locationId –∏–∑ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
        
        page = 0
        num_per_page = 50  # –ú–∞–∫—Å–∏–º—É–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞ –∑–∞–ø—Ä–æ—Å
        
        try:
            while len(listings_data) < max_results:
                headers = {
                    'User-Agent': UserAgent().random,
                    'Accept': '*/*',
                    'Accept-Language': 'en-US,en;q=0.5',
                    'Referer': f'https://www.compass.com/homes-for-sale/{location}/',
                    'Content-Type': 'application/json',
                    'Origin': 'https://www.compass.com',
                    'Sec-GPC': '1',
                    'Connection': 'keep-alive',
                    'Sec-Fetch-Dest': 'empty',
                    'Sec-Fetch-Mode': 'cors',
                    'Sec-Fetch-Site': 'same-origin',
                    'Priority': 'u=6',
                }
                
                params = {
                    'searchQuery': '{"sort":{"column":"dom","direction":"asc"}}',
                }
                
                json_data = {
                    'searchResultId': search_result_id,
                    'rawLolSearchQuery': {
                        'listingTypes': [2],  # 2 = For Sale
                        'nePoint': ne_point,
                        'swPoint': sw_point,
                        'saleStatuses': [12, 9],  # Active listings
                        'num': num_per_page,
                        'start': page * num_per_page,
                        'sortOrder': 46,  # DOM ascending
                        'facetFieldNames': [
                            'contributingDatasetList',
                            'compassListingTypes',
                            'comingSoon',
                        ],
                    },
                    'viewport': {
                        'northeast': viewport_ne,
                        'southwest': viewport_sw,
                    },
                    'viewportFrom': 'response',
                    'height': 1350,
                    'width': 1253,
                    'isMapFullyInitialized': True,
                    'purpose': 'search',
                }
                
                api_url = f"{self.base_url}/homes-for-sale/{location}/mapview={viewport_ne['lat']},{viewport_ne['lng']},{viewport_sw['lat']},{viewport_sw['lng']}/"
                
                logger.info(f"[1-API] –ó–∞–ø—Ä–æ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page + 1}, start={page * num_per_page}")
                
                try:
                    response = requests.post(
                        api_url,
                        params=params,
                        json=json_data,
                        headers=headers,
                        timeout=30
                    )
                    response.raise_for_status()
                    
                    data = response.json()
                    
                    if 'lolResults' not in data or 'data' not in data['lolResults']:
                        logger.warning(f"[1-API] –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ API")
                        break
                    
                    listings = data['lolResults']['data']
                    total_items = data['lolResults'].get('totalItems', 0)
                    
                    logger.info(f"[1-API] –ü–æ–ª—É—á–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–≤—Å–µ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ: {total_items})")
                    
                    if not listings:
                        logger.info(f"[1-API] –ë–æ–ª—å—à–µ –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –ó–∞–≤–µ—Ä—à–∞–µ–º.")
                        break
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–≤–∫–ª—é—á–∞—è listing –æ–±—ä–µ–∫—Ç—ã)
                    for item in listings:
                        listing = item.get('listing', {})
                        if listing:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ listing
                            listings_data.append(listing)
                    
                    logger.info(f"[1-API] –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page + 1} –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –í—Å–µ–≥–æ: {len(listings_data)}")
                    
                    # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –º–µ–Ω—å—à–µ, —á–µ–º –∑–∞–ø—Ä–∞—à–∏–≤–∞–ª–∏, –∏–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏–º–∏—Ç–∞, –∑–∞–≤–µ—Ä—à–∞–µ–º
                    if len(listings) < num_per_page or len(listings_data) >= max_results:
                        logger.info(f"[1-API] –ü–æ–ª—É—á–µ–Ω–æ –º–µ–Ω—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç. –ó–∞–≤–µ—Ä—à–∞–µ–º.")
                        break
                    
                    # –ï—Å–ª–∏ total_items –º–µ–Ω—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ —Ç–µ–∫—É—â–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É, –∑–∞–≤–µ—Ä—à–∞–µ–º
                    if total_items > 0 and len(listings_data) >= total_items:
                        logger.info(f"[1-API] –ü–æ–ª—É—á–µ–Ω—ã –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è ({total_items}). –ó–∞–≤–µ—Ä—à–∞–µ–º.")
                        break
                    
                    page += 1
                    
                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    time.sleep(1)
                    
                except requests.exceptions.RequestException as e:
                    logger.error(f"[1-API] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API: {e}")
                    break
                except json.JSONDecodeError as e:
                    logger.error(f"[1-API] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ JSON –æ—Ç–≤–µ—Ç–∞: {e}")
                    break
            
            logger.info(f"[1-API] –ò—Ç–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ {len(listings_data)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            return listings_data[:max_results]
            
        except Exception as e:
            logger.error(f"[1-API] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ API: {e}")
            import traceback
            traceback.print_exc()
            return listings_data


    # ---------------------- –≠–¢–ê–ü 3: –ü–ê–†–°–ò–ù–ì –î–ê–ù–ù–´–• ----------------------

    def get_listing_html(self, url: str) -> str | None:
        """–ü–æ–ª—É—á–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—ä—è–≤–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ requests"""
        try:
            headers = {
                'User-Agent': UserAgent().random,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Referer': 'https://www.compass.com/',
                'Connection': 'keep-alive',
            }
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            return response.text
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å HTML –¥–ª—è {url}: {e}")
            return None

    def parse_listing_from_api_data(self, listing_data: dict) -> DbDTO | None:
        """
        –≠–¢–ê–ü 3 (API): –ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–∞–ø—Ä—è–º—É—é –∏–∑ API –æ—Ç–≤–µ—Ç–∞
        –î–ª—è –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –ø–æ–ª–µ–π (–æ–ø–∏—Å–∞–Ω–∏–µ, MLS, brochure, –∞–≥–µ–Ω—Ç—ã) –∑–∞–≥—Ä—É–∂–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        """
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∏ ID
            page_link = listing_data.get('pageLink') or listing_data.get('navigationPageLink', '')
            if not page_link:
                logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω pageLink –≤ –¥–∞–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
                return None
            
            if not page_link.startswith('http'):
                url = urljoin(self.base_url, page_link)
            else:
                url = page_link
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ URL
            listing_id = "unknown"
            parsed = urlparse(url)
            path_parts = [p for p in parsed.path.split('/') if p]
            if path_parts:
                listing_id = path_parts[-1]
            
            # –°–Ω–∞—á–∞–ª–∞ –º–∞–ø–ø–∏–º –¥–∞–Ω–Ω—ã–µ –∏–∑ API
            dto = self._map_to_dto_from_api(listing_data, url, listing_id)
            
            # –ó–∞—Ç–µ–º –∑–∞–≥—Ä—É–∂–∞–µ–º HTML –¥–ª—è –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –ø–æ–ª–µ–π
            html = self.get_listing_html(url)
            if html:
                # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ HTML
                self._enrich_from_html(dto, html, listing_id)
            
            return dto
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –¥–∞–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
            import traceback
            traceback.print_exc()
            return None


    def _map_to_dto_from_api(self, listing_data: dict, url: str, listing_id: str) -> DbDTO:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ API –æ—Ç–≤–µ—Ç–∞ –≤ DbDTO"""
        
        # Helper –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π
        def get_val(obj, path, default=None):
            for key in path.split('.'):
                if isinstance(obj, dict) and key in obj:
                    obj = obj[key]
                else:
                    return default
            return obj
        
        # Address - –∏–∑ structuredData.singleFamilyResidence (JSON —Å—Ç—Ä–æ–∫–∞)
        address = "Address not found"
        structured_data = listing_data.get('structuredData', {})
        if 'singleFamilyResidence' in structured_data:
            try:
                sfr_str = structured_data['singleFamilyResidence']
                if isinstance(sfr_str, str):
                    sfr = json.loads(sfr_str)
                    if 'address' in sfr:
                        addr = sfr['address']
                        street = addr.get('streetAddress', '')
                        city = addr.get('addressLocality', '')
                        state = addr.get('addressRegion', '')
                        zip_code = addr.get('postalCode', '')
                        parts = [p for p in [street, city, state, zip_code] if p]
                        address = ', '.join(parts) if parts else "Address not found"
            except (json.JSONDecodeError, KeyError):
                pass
        
        # –ï—Å–ª–∏ –∞–¥—Ä–µ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º –∏–∑ –¥—Ä—É–≥–∏—Ö –º–µ—Å—Ç
        if address == "Address not found":
            location = listing_data.get('location', {})
            if isinstance(location, dict) and 'prettyAddress' in location:
                address = location['prettyAddress']
        
        # Price - –∏–∑ title (–Ω–∞–ø—Ä–∏–º–µ—Ä "$2,300,000")
        title = listing_data.get('title', '')
        sale_price = None
        lease_price = None
        listing_type = 'sale'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        if title and title.startswith('$'):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ –∏–∑ —Å—Ç—Ä–æ–∫–∏ —Ç–∏–ø–∞ "$2,300,000"
            price_match = re.search(r'\$([\d,]+)', title.replace(',', ''))
            if price_match:
                price_str = title  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–æ listingType –∏–ª–∏ –¥—Ä—É–≥–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
                listing_type_code = listing_data.get('status')  # status –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ —Ç–∏–ø
                # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è homes-for-sale —ç—Ç–æ –ø—Ä–æ–¥–∞–∂–∞
                sale_price = price_str
                listing_type = 'sale'
        
        # Size - –∏–∑ subStats (–ø–ª–æ—â–∞–¥—å)
        size = None
        sub_stats = listing_data.get('subStats', [])
        for stat in sub_stats:
            title_key = stat.get('title', '')
            subtitle = stat.get('subtitle', '').replace(',', '').replace('-', '').strip()
            
            if title_key == 'sqft' and subtitle and subtitle != 'Unavailable':
                try:
                    sqft_num = int(subtitle)
                    size = f"{sqft_num:,} SF"
                except ValueError:
                    size = subtitle + " SF"
                break
            elif title_key == 'acres' and subtitle and subtitle != 'Unavailable' and not size:
                # –ï—Å–ª–∏ –Ω–µ—Ç sqft, –∏—Å–ø–æ–ª—å–∑—É–µ–º acres
                size = f"{subtitle} acres"
        
        # Description - –∏–∑ structuredData.product (–∫–æ—Ä–æ—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ)
        # –ü–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ HTML
        description = None
        structured_data = listing_data.get('structuredData', {})
        if 'product' in structured_data:
            try:
                product_str = structured_data['product']
                if isinstance(product_str, str):
                    product = json.loads(product_str)
                    description = product.get('description', '')
            except (json.JSONDecodeError, KeyError):
                pass
        
        # Photos - –∏–∑ media
        photos = []
        media = listing_data.get('media', [])
        for item in media:
            # –í API media —Å–æ–¥–µ—Ä–∂–∏—Ç originalUrl –∏ thumbnailUrl
            if 'originalUrl' in item:
                photos.append(item['originalUrl'])
            elif 'thumbnailUrl' in item:
                photos.append(item['thumbnailUrl'])
            elif 'url' in item:
                photos.append(item['url'])
        
        # Status - –∏–∑ badges –∏–ª–∏ status
        status = "Available"
        badges = listing_data.get('badges', {})
        corner_badges = badges.get('cornerBadges', [])
        if corner_badges:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π badge (–æ–±—ã—á–Ω–æ —ç—Ç–æ —Å—Ç–∞—Ç—É—Å —Ç–∏–ø–∞ "Coming Soon")
            status = corner_badges[0].get('displayText', 'Available')
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç badges, –∏—Å–ø–æ–ª—å–∑—É–µ–º status –∫–æ–¥
            status_code = listing_data.get('status')
            if status_code:
                status_map = {
                    12: "Active",
                    9: "Active",
                    14: "Active",
                    # –î–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –∫–æ–¥—ã –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                }
                status = status_map.get(status_code, f"Status {status_code}")
        
        # Details - –∏–∑ subStats –∏ –¥—Ä—É–≥–∏—Ö –ø–æ–ª–µ–π (extract_details)
        details = {}
        
        # Bedrooms, Bathrooms –∏–∑ subStats
        for stat in sub_stats:
            title_key = stat.get('title', '')
            subtitle = stat.get('subtitle', '').replace('-', '').strip()
            
            if title_key == 'beds' and subtitle and subtitle != 'Unavailable':
                try:
                    details['bedrooms'] = int(subtitle)
                except ValueError:
                    details['bedrooms'] = subtitle
            elif title_key == 'baths' and subtitle and subtitle != 'Unavailable':
                try:
                    details['bathrooms'] = float(subtitle) if '.' in subtitle else int(subtitle)
                except ValueError:
                    details['bathrooms'] = subtitle
            elif title_key == 'acres' and subtitle and subtitle != 'Unavailable':
                details['acres'] = subtitle
            elif title_key == 'sqft' and subtitle and subtitle != 'Unavailable':
                try:
                    details['square_feet'] = int(subtitle.replace(',', ''))
                except ValueError:
                    details['square_feet'] = subtitle
        
        # Property type –∏–∑ clusterSummary
        cluster_summary = listing_data.get('clusterSummary', {})
        if 'propertyType' in cluster_summary:
            prop_type = cluster_summary['propertyType']
            if isinstance(prop_type, dict) and 'masterType' in prop_type:
                master_type = prop_type['masterType']
                if isinstance(master_type, dict) and 'GLOBAL' in master_type:
                    types_list = master_type['GLOBAL']
                    if types_list:
                        details['property_type'] = types_list[0]
        
        # Price range –∏–∑ clusterSummary
        if 'priceRange' in cluster_summary:
            price_range = cluster_summary['priceRange']
            if isinstance(price_range, list) and price_range:
                details['price_range'] = price_range
        
        # Agents - –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –∏–∑ HTML
        agents = []
        
        # MLS number - –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –∏–∑ HTML (extract_mls)
        mls_number = None
        
        return DbDTO(
            source_name=self.source_name,
            listing_id=listing_id,
            listing_link=url,
            listing_type=listing_type,
            listing_status=status,
            address=address,
            sale_price=sale_price,
            lease_price=lease_price,
            size=size,
            property_description=description,
            listing_details=details if details else None,
            photos=photos if photos else None,
            brochure_pdf=None,
            mls_number=mls_number,
            agents=agents if agents else None,
            agency_phone=None,
        )

    def _enrich_from_html(self, dto: DbDTO, html: str, listing_id: str) -> None:
        """–î–æ–ø–æ–ª–Ω—è–µ—Ç DTO –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–æ–ø–∏—Å–∞–Ω–∏–µ, MLS, brochure, –∞–≥–µ–Ω—Ç—ã)"""
        try:
            soup = BeautifulSoup(html, 'lxml')
            
            # 1. Extract MLS number
            mls = self.extract_mls(soup)
            if mls:
                dto.mls_number = mls
            
            # 2. Extract description
            description = self.extract_description(soup)
            if description and not dto.property_description:
                dto.property_description = description
            
            # 3. Extract brochure PDF
            brochure = self.extract_brochure_pdf(soup)
            if brochure:
                dto.brochure_pdf = brochure
            
            # 4. Extract agents
            agents = self.extract_agents(soup, self.base_url)
            if agents:
                dto.agents = agents
            
            # 5. Extract additional details
            additional_details = self.extract_details(soup)
            if additional_details:
                if dto.listing_details:
                    dto.listing_details.update(additional_details)
                else:
                    dto.listing_details = additional_details
            
            # 6. Extract size –µ—Å–ª–∏ –Ω–µ –±—ã–ª–æ –Ω–∞–π–¥–µ–Ω–æ
            if not dto.size:
                size = self.extract_size(soup)
                if size:
                    dto.size = size
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ HTML: {e}")

    @staticmethod
    def extract_mls(soup: BeautifulSoup) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç MLS –Ω–æ–º–µ—Ä –∏–∑ HTML"""
        mls_patterns = [
            re.compile(r'MLS[#:\s]*([A-Z0-9\-]+)', re.I),
            re.compile(r'MLS\s*Number[#:\s]*([A-Z0-9\-]+)', re.I),
            re.compile(r'Multiple\s*Listing\s*Service[#:\s]*([A-Z0-9\-]+)', re.I),
        ]
        
        page_text = soup.get_text()
        for pattern in mls_patterns:
            match = pattern.search(page_text)
            if match:
                return match.group(1).strip()
        
        # –ò—â–µ–º –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        scripts = soup.find_all('script', type='application/ld+json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, dict):
                    # –ò—â–µ–º MLS –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
                    if 'identifier' in data:
                        identifier = data['identifier']
                        if isinstance(identifier, dict) and identifier.get('@type') == 'PropertyValue':
                            value = identifier.get('value', '')
                            if 'MLS' in value.upper():
                                return value
            except (json.JSONDecodeError, AttributeError):
                continue
        
        return None

    @staticmethod
    def extract_description(soup: BeautifulSoup) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ HTML"""
        # –ò—â–µ–º –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
        selectors = [
            'div[class*="description"]',
            'div[class*="property-description"]',
            'div[class*="listing-description"]',
            'section[class*="description"]',
            '[data-testid*="description"]',
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text(strip=True)
                if text and len(text) > 50:  # –ú–∏–Ω–∏–º—É–º 50 —Å–∏–º–≤–æ–ª–æ–≤
                    return text
        
        # –ò—â–µ–º –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        scripts = soup.find_all('script', type='application/ld+json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, dict):
                    if 'description' in data:
                        desc = data['description']
                        if isinstance(desc, str) and len(desc) > 50:
                            return desc
            except (json.JSONDecodeError, AttributeError):
                continue
        
        return None

    @staticmethod
    def extract_brochure_pdf(soup: BeautifulSoup) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ brochure PDF"""
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ PDF
        pdf_links = soup.find_all('a', href=re.compile(r'\.pdf$', re.I))
        for link in pdf_links:
            href = link.get('href', '')
            text = link.get_text(strip=True).lower()
            if 'brochure' in text or 'flyer' in text or 'marketing' in text:
                if not href.startswith('http'):
                    href = urljoin('https://www.compass.com', href)
                return href
        
        # –ò—â–µ–º –≤ data-–∞—Ç—Ä–∏–±—É—Ç–∞—Ö
        elements = soup.find_all(attrs={'data-brochure': True})
        for elem in elements:
            brochure_url = elem.get('data-brochure')
            if brochure_url:
                if not brochure_url.startswith('http'):
                    brochure_url = urljoin('https://www.compass.com', brochure_url)
                return brochure_url
        
        return None

    @staticmethod
    def extract_agents(soup: BeautifulSoup, base_url: str) -> list[AgentData]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–≥–µ–Ω—Ç–æ–≤ –∏–∑ HTML"""
        agents = []
        
        # –ò—â–µ–º –±–ª–æ–∫–∏ —Å –∞–≥–µ–Ω—Ç–∞–º–∏
        agent_selectors = [
            '[class*="agent"]',
            '[class*="listing-agent"]',
            '[data-testid*="agent"]',
            '[class*="broker"]',
        ]
        
        for selector in agent_selectors:
            elements = soup.select(selector)
            for elem in elements:
                # –ò—â–µ–º –∏–º—è
                name_elem = elem.find(['h3', 'h4', 'h5', 'div'], class_=re.compile(r'name|agent', re.I))
                if not name_elem:
                    name_elem = elem.find('a', href=re.compile(r'/agent/|/team/'))
                
                if name_elem:
                    name = name_elem.get_text(strip=True)
                    if name and len(name) > 2:
                        # –ò—â–µ–º —Å—Å—ã–ª–∫—É
                        link = None
                        a_tag = name_elem.find('a') if name_elem.name != 'a' else name_elem
                        if a_tag:
                            link = a_tag.get('href', '')
                            if link and not link.startswith('http'):
                                link = urljoin(base_url, link)
                        
                        # –ò—â–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω
                        phone = None
                        phone_elem = elem.find('a', href=re.compile(r'tel:'))
                        if phone_elem:
                            phone_match = re.search(r'tel:([\d\s\-\(\)]+)', phone_elem.get('href', ''))
                            if phone_match:
                                phone = phone_match.group(1).strip()
                        
                        # –ò—â–µ–º email
                        email = None
                        email_elem = elem.find('a', href=re.compile(r'mailto:'))
                        if email_elem:
                            email_match = re.search(r'mailto:([^\s]+)', email_elem.get('href', ''))
                            if email_match:
                                email = email_match.group(1).strip()
                        
                        # –ò—â–µ–º —Ñ–æ—Ç–æ
                        photo_url = None
                        img = elem.find('img')
                        if img:
                            photo_url = img.get('src') or img.get('data-src')
                            if photo_url and not photo_url.startswith('http'):
                                photo_url = urljoin(base_url, photo_url)
                        
                        # –ò—â–µ–º –¥–æ–ª–∂–Ω–æ—Å—Ç—å
                        title = None
                        title_elem = elem.find(['div', 'span'], class_=re.compile(r'title|position|role', re.I))
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                        
                        agent = AgentData(
                            name=name,
                            title=title,
                            phone_primary=phone,
                            email=email,
                            photo_url=photo_url,
                            social_media=link,
                        )
                        agents.append(agent)
        
        return agents

    @staticmethod
    def extract_details(soup: BeautifulSoup) -> dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è (—Ç–∞–±–ª–∏—Ü–∞) –∏–∑ HTML"""
        details = {}
        
        # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—ã —Å –¥–µ—Ç–∞–ª—è–º–∏
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    key = cells[0].get_text(strip=True)
                    value = cells[1].get_text(strip=True)
                    if key and value:
                        details[key.lower().replace(' ', '_')] = value
        
        # –ò—â–µ–º —Å–ø–∏—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π (dl)
        dl_elements = soup.find_all('dl')
        for dl in dl_elements:
            dts = dl.find_all('dt')
            dds = dl.find_all('dd')
            for dt, dd in zip(dts, dds):
                key = dt.get_text(strip=True)
                value = dd.get_text(strip=True)
                if key and value:
                    details[key.lower().replace(' ', '_')] = value
        
        # –ò—â–µ–º div —Å –ø–∞—Ä–∞–º–∏ –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ
        detail_divs = soup.find_all(['div', 'section'], class_=re.compile(r'detail|feature|spec', re.I))
        for div in detail_divs:
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω "Key: Value"
            text = div.get_text()
            matches = re.findall(r'([^:]+):\s*([^\n]+)', text)
            for key, value in matches:
                key = key.strip().lower().replace(' ', '_')
                value = value.strip()
                if key and value and len(key) < 50:
                    details[key] = value
        
        return details

    @staticmethod
    def extract_size(soup: BeautifulSoup) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–ª–æ—â–∞–¥—å –∏–∑ HTML (–µ—Å–ª–∏ –Ω–µ –±—ã–ª–æ –Ω–∞–π–¥–µ–Ω–æ –≤ API)"""
        # –ò—â–µ–º –ø–ª–æ—â–∞–¥—å –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        size_patterns = [
            re.compile(r'(\d{1,3}(?:,\d{3})*)\s*(?:sq\.?\s*ft\.?|SF|square\s*feet)', re.I),
            re.compile(r'(\d+\.?\d*)\s*acres?', re.I),
        ]
        
        page_text = soup.get_text()
        for pattern in size_patterns:
            match = pattern.search(page_text)
            if match:
                size_val = match.group(1)
                if 'acre' in pattern.pattern.lower():
                    return f"{size_val} acres"
                else:
                    return f"{size_val.replace(',', '')} SF"
        
        return None

    def _save_html_if_needed(self, html: str, listing_id: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç HTML –≤ —Ñ–∞–π–ª"""
        self.html_counter += 1
        if self.html_counter % self.save_html_every == 0:
            safe_filename = re.sub(r'[^\w\-_\.]', '_', listing_id)
            if not safe_filename or safe_filename == '_':
                safe_filename = f"listing_{self.html_counter}"
            filepath = os.path.join(self.html_save_dir, f"{safe_filename}.html")
            try:
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(html)
                logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω HTML [{self.html_counter}]: {filepath}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ HTML: {e}")

    def run(self, location: str = "new-york", max_results: int = 1000) -> list[DbDTO]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ API"""
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ API
        logger.info("[API MODE] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö")
        listings_data = self.get_listings_from_api(location, max_results)
        
        if not listings_data:
            logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ API")
            return []
        
        # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ –∏–∑ API –æ—Ç–≤–µ—Ç–∞
        results = []
        logger.info(f"[2-3] –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(listings_data)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ API...")
        
        for i, listing_data in enumerate(listings_data):
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ [{i+1}/{len(listings_data)}]...")
            dto = self.parse_listing_from_api_data(listing_data)
            if dto:
                results.append(dto)
                logger.info(f"‚úì –£—Å–ø–µ—à–Ω–æ: {dto.address}")
            else:
                logger.warning(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏–µ {i+1}")
        
        logger.info(f"\n–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(results)}/{len(listings_data)}")
        return results

if __name__ == '__main__':
    parser = CompassParser()
    parser.run("new-york", 5)
