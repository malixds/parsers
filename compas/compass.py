import asyncio
import json
import logging
import os
import re
import time
import uuid
from typing import Any
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager

from schema import DbDTO, AgentData

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CompassParser:
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è compass.com —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Selenium
    1. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ –±—Ä–∞—É–∑–µ—Ä (–æ–±—Ö–æ–¥ –∑–∞—â–∏—Ç—ã)
    2. –ü–æ–ª—É—á–µ–Ω–∏–µ HTML –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ª–∏—Å—Ç–∏–Ω–≥–∞
    3. –ü–∞—Ä—Å–∏–Ω–≥ window.__INITIAL_DATA__ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
    """
    
    def __init__(
        self,
        headless: bool = True,
        save_html_every: int = 20,
        html_save_dir: str = "htmls",
        page_load_timeout: int = 30,
    ) -> None:
        self.source_name = "compass"
        self.base_url = "https://www.compass.com"
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Selenium
        self.chrome_options = Options()
        if headless:
            self.chrome_options.add_argument("--headless=new")
        
        # –ë–∞–∑–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        self.chrome_options.add_argument("--no-sandbox")
        self.chrome_options.add_argument("--disable-dev-shm-usage")
        self.chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–∞—Å–∫–∏—Ä–æ–≤–∫–∞ –±—Ä–∞—É–∑–µ—Ä–∞
        self.chrome_options.add_argument("--disable-web-security")
        self.chrome_options.add_argument("--disable-features=IsolateOrigins,site-per-process")
        self.chrome_options.add_argument("--lang=en-US,en")
        self.chrome_options.add_argument("--window-size=1920,1080")
        
        # –ê–∫—Ç—É–∞–ª—å–Ω—ã–π User-Agent
        self.chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36")
        
        # –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
        self.chrome_options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
        self.chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã
        prefs = {
            "profile.default_content_setting_values": {
                "notifications": 2,
                "geolocation": 2,
            },
            "profile.managed_default_content_settings": {
                "images": 1
            }
        }
        self.chrome_options.add_experimental_option("prefs", prefs)
        
        self.page_load_timeout = page_load_timeout
        self.driver = None
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML
        self.save_html_every = save_html_every
        self.html_save_dir = html_save_dir
        self.html_counter = 0
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        if not os.path.exists(self.html_save_dir):
            os.makedirs(self.html_save_dir)
            logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML: {self.html_save_dir}")

    def start_driver(self):
        """–ó–∞–ø—É—Å–∫ –¥—Ä–∞–π–≤–µ—Ä–∞"""
        if not self.driver:
            logger.info("–ó–∞–ø—É—Å–∫ Chrome driver...")
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=self.chrome_options)
            self.driver.set_page_load_timeout(self.page_load_timeout)
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –º–∞—Å–∫–∏—Ä–æ–≤–∫–∞ webdriver
            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                    window.navigator.chrome = {
                        runtime: {}
                    };
                    Object.defineProperty(navigator, 'languages', {
                        get: () => ['en-US', 'en']
                    });
                    Object.defineProperty(navigator, 'plugins', {
                        get: () => [1, 2, 3, 4, 5]
                    });
                '''
            })
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —á–µ—Ä–µ–∑ CDP
            self.driver.execute_cdp_cmd('Network.setUserAgentOverride', {
                "userAgent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
                "acceptLanguage": "en-US,en;q=0.9",
                "platform": "Win32"
            })

    def stop_driver(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥—Ä–∞–π–≤–µ—Ä–∞"""
        if self.driver:
            logger.info("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ Chrome driver...")
            self.driver.quit()
            self.driver = None

    def get_page_source(self, url: str) -> str | None:
        """–ü–æ–ª—É—á–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ Selenium"""
        if not self.driver:
            self.start_driver()
        
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
            self.driver.get(url)
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            try:
                WebDriverWait(self.driver, 15).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                # –î–∞–µ–º –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ JS —Å–∫—Ä–∏–ø—Ç–æ–≤ –∏ –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö
                time.sleep(5)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ –æ—à–∏–±–∫—É CloudFront
                page_source = self.driver.page_source
                if "403 ERROR" in page_source or "The request could not be satisfied" in page_source:
                    logger.error(f"CloudFront –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –∑–∞–ø—Ä–æ—Å –¥–ª—è {url}")
                    return None
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å (–µ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç)
                if len(page_source) < 1000:
                    logger.warning(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è ({len(page_source)} —Å–∏–º–≤–æ–ª–æ–≤), –≤–æ–∑–º–æ–∂–Ω–æ –æ—à–∏–±–∫–∞")
                    return None
                
                return page_source
            except Exception as wait_error:
                logger.warning(f"–¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {wait_error}")
                # –í—Å–µ —Ä–∞–≤–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º page_source, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                page_source = self.driver.page_source
                if "403 ERROR" in page_source or "The request could not be satisfied" in page_source:
                    return None
                return page_source if len(page_source) > 1000 else None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
            # –ï—Å–ª–∏ –¥—Ä–∞–π–≤–µ—Ä —É–ø–∞–ª, –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏–º –µ–≥–æ
            try:
                self.stop_driver()
            except:
                pass
            return None

    # ---------------------- –≠–¢–ê–ü 1: –ò–ù–î–ï–ö–°–ê–¶–ò–Ø ----------------------

    def get_listings_from_api(self, location: str = "new-york", max_results: int = 1000) -> list[dict]:
        """
        –≠–¢–ê–ü 1 (API): –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ API compass.com
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–≤–∫–ª—é—á–∞—è listing –æ–±—ä–µ–∫—Ç—ã)
        """
        logger.info(f"[1-API] –ü–æ–ª—É—á–∞—é –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ API –¥–ª—è –ª–æ–∫–∞—Ü–∏–∏: {location}")
        
        listings_data = []
        search_result_id = str(uuid.uuid4())
        
        # –ë–∞–∑–æ–≤—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è New York (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –¥–ª—è –¥—Ä—É–≥–∏—Ö –ª–æ–∫–∞—Ü–∏–π)
        # –≠—Ç–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ø–æ–∫—Ä—ã–≤–∞—é—Ç –≤–µ—Å—å —à—Ç–∞—Ç NY
        ne_point = {"latitude": 45.3525295, "longitude": -72.3285732}
        sw_point = {"latitude": 39.9017281, "longitude": -79.2115078}
        viewport_ne = {"lat": 45.2954092, "lng": -72.3285732}
        viewport_sw = {"lat": 39.839376, "lng": -79.2115078}
        
        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å locationId –∏–∑ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
        
        page = 0
        num_per_page = 50  # –ú–∞–∫—Å–∏–º—É–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞ –∑–∞–ø—Ä–æ—Å
        
        try:
            while len(listings_data) < max_results:
                headers = {
                    'User-Agent': UserAgent().random,
                    'Accept': '*/*',
                    'Accept-Language': 'en-US,en;q=0.5',
                    'Referer': f'https://www.compass.com/homes-for-sale/{location}/',
                    'Content-Type': 'application/json',
                    'Origin': 'https://www.compass.com',
                    'Sec-GPC': '1',
                    'Connection': 'keep-alive',
                    'Sec-Fetch-Dest': 'empty',
                    'Sec-Fetch-Mode': 'cors',
                    'Sec-Fetch-Site': 'same-origin',
                    'Priority': 'u=6',
                }
                
                params = {
                    'searchQuery': '{"sort":{"column":"dom","direction":"asc"}}',
                }
                
                json_data = {
                    'searchResultId': search_result_id,
                    'rawLolSearchQuery': {
                        'listingTypes': [2],  # 2 = For Sale
                        'nePoint': ne_point,
                        'swPoint': sw_point,
                        'saleStatuses': [12, 9],  # Active listings
                        'num': num_per_page,
                        'start': page * num_per_page,
                        'sortOrder': 46,  # DOM ascending
                        'facetFieldNames': [
                            'contributingDatasetList',
                            'compassListingTypes',
                            'comingSoon',
                        ],
                    },
                    'viewport': {
                        'northeast': viewport_ne,
                        'southwest': viewport_sw,
                    },
                    'viewportFrom': 'response',
                    'height': 1350,
                    'width': 1253,
                    'isMapFullyInitialized': True,
                    'purpose': 'search',
                }
                
                api_url = f"{self.base_url}/homes-for-sale/{location}/mapview={viewport_ne['lat']},{viewport_ne['lng']},{viewport_sw['lat']},{viewport_sw['lng']}/"
                
                logger.info(f"[1-API] –ó–∞–ø—Ä–æ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page + 1}, start={page * num_per_page}")
                
                try:
                    response = requests.post(
                        api_url,
                        params=params,
                        json=json_data,
                        headers=headers,
                        timeout=30
                    )
                    response.raise_for_status()
                    
                    data = response.json()
                    
                    if 'lolResults' not in data or 'data' not in data['lolResults']:
                        logger.warning(f"[1-API] –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ API")
                        break
                    
                    listings = data['lolResults']['data']
                    total_items = data['lolResults'].get('totalItems', 0)
                    
                    logger.info(f"[1-API] –ü–æ–ª—É—á–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–≤—Å–µ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ: {total_items})")
                    
                    if not listings:
                        logger.info(f"[1-API] –ë–æ–ª—å—à–µ –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –ó–∞–≤–µ—Ä—à–∞–µ–º.")
                        break
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–≤–∫–ª—é—á–∞—è listing –æ–±—ä–µ–∫—Ç—ã)
                    for item in listings:
                        listing = item.get('listing', {})
                        if listing:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ listing
                            listings_data.append(listing)
                    
                    logger.info(f"[1-API] –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page + 1} –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –í—Å–µ–≥–æ: {len(listings_data)}")
                    
                    # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –º–µ–Ω—å—à–µ, —á–µ–º –∑–∞–ø—Ä–∞—à–∏–≤–∞–ª–∏, –∏–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏–º–∏—Ç–∞, –∑–∞–≤–µ—Ä—à–∞–µ–º
                    if len(listings) < num_per_page or len(listings_data) >= max_results:
                        logger.info(f"[1-API] –ü–æ–ª—É—á–µ–Ω–æ –º–µ–Ω—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç. –ó–∞–≤–µ—Ä—à–∞–µ–º.")
                        break
                    
                    # –ï—Å–ª–∏ total_items –º–µ–Ω—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ —Ç–µ–∫—É—â–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É, –∑–∞–≤–µ—Ä—à–∞–µ–º
                    if total_items > 0 and len(listings_data) >= total_items:
                        logger.info(f"[1-API] –ü–æ–ª—É—á–µ–Ω—ã –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è ({total_items}). –ó–∞–≤–µ—Ä—à–∞–µ–º.")
                        break
                    
                    page += 1
                    
                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    time.sleep(1)
                    
                except requests.exceptions.RequestException as e:
                    logger.error(f"[1-API] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API: {e}")
                    break
                except json.JSONDecodeError as e:
                    logger.error(f"[1-API] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ JSON –æ—Ç–≤–µ—Ç–∞: {e}")
                    break
            
            logger.info(f"[1-API] –ò—Ç–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ {len(listings_data)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            return listings_data[:max_results]
            
        except Exception as e:
            logger.error(f"[1-API] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ API: {e}")
            import traceback
            traceback.print_exc()
            return listings_data

    def get_listing_urls_from_search(self, location: str = "new-york", max_results: int = 1000) -> list[str]:
        """
        –≠–¢–ê–ü 1: –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–∏—Å–∫–∞ (Selenium)
        """
        logger.info(f"[1] –ü–æ–ª—É—á–∞—é —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–ª—è –ª–æ–∫–∞—Ü–∏–∏: {location}")
        
        if not self.driver:
            self.start_driver()
            
        urls = []
        page = 1
        
        try:
            while len(urls) < max_results:
                # URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞
                search_url = f"{self.base_url}/homes-for-sale/{location}/"
                if page > 1:
                    search_url += f"?page={page}"
                
                logger.info(f"[1] –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {search_url}")
                
                html = self.get_page_source(search_url)
                if not html:
                    logger.warning(f"[1] –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}")
                    break
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –ø–æ–∏—Å–∫–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                if page == 1:
                    debug_file = os.path.join(self.html_save_dir, f"search_page_{page}_selenium.html")
                    with open(debug_file, 'w', encoding='utf-8') as f:
                        f.write(html)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞
                if "403 ERROR" in html or "The request could not be satisfied" in html:
                    logger.error(f"[1] ‚ö†Ô∏è  CloudFront –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –¥–æ—Å—Ç—É–ø –∫ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}!")
                    logger.error(f"[1] –í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è:")
                    logger.error(f"[1]   1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ VPN –∏–ª–∏ –ø—Ä–æ–∫—Å–∏")
                    logger.error(f"[1]   2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –±–µ–∑ headless —Ä–µ–∂–∏–º–∞ (headless=False)")
                    logger.error(f"[1]   3. –£–≤–µ–ª–∏—á—å—Ç–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏")
                    break
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏
                new_urls = self._extract_urls_from_html(html)
                logger.info(f"[1] –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(new_urls)} —Å—Å—ã–ª–æ–∫ –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}")
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ
                page_urls = [url for url in new_urls if url not in urls]
                
                if not page_urls:
                    logger.info(f"[1] –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –ó–∞–≤–µ—Ä—à–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥.")
                    # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–±–ª–µ–º–∞ —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º
                    if page == 1:
                        logger.warning(f"[1] ‚ö†Ô∏è  –ù–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π!")
                        logger.warning(f"[1] –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π HTML —Ñ–∞–π–ª: {os.path.join(self.html_save_dir, f'search_page_{page}_selenium.html')}")
                    break
                
                urls.extend(page_urls)
                logger.info(f"[1] –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} –Ω–∞–π–¥–µ–Ω–æ {len(page_urls)} –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –í—Å–µ–≥–æ: {len(urls)}")
                
                # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                page += 1
                
                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
                if page > 50:
                    logger.warning(f"[1] –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü (50). –û—Å—Ç–∞–Ω–æ–≤–∫–∞.")
                    break
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
                time.sleep(2)
            
            logger.info(f"[1] –ò—Ç–æ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(urls)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
            return urls[:max_results]
            
        except Exception as e:
            logger.error(f"[1] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {e}")
            import traceback
            traceback.print_exc()
            return urls

    def _extract_urls_from_html(self, html: str) -> list[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ HTML"""
        urls = []
        soup = BeautifulSoup(html, 'lxml')
        
        # 1. –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —á–µ—Ä–µ–∑ __INITIAL_DATA__
        initial_data = self.extract_initial_data(html)
        if initial_data:
            logger.debug("–ù–∞–π–¥–µ–Ω __INITIAL_DATA__, –∏–∑–≤–ª–µ–∫–∞—é –æ–±—ä—è–≤–ª–µ–Ω–∏—è...")
            listings = self.extract_listings_from_initial_data(initial_data)
            logger.debug(f"–ù–∞–π–¥–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –≤ __INITIAL_DATA__")
            
            for listing in listings:
                url = None
                if isinstance(listing, dict):
                    # –ü—Ä–æ–±—É–µ–º —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å URL –∏–∑ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª–µ–π
                    listing_id = (
                        listing.get('id') or 
                        listing.get('listingId') or 
                        listing.get('mlsNumber') or
                        listing.get('listingIdSHA')
                    )
                    
                    # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å URL –∏–∑ location (–∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
                    if not url and listing.get('location') and listing.get('location', {}).get('seoId'):
                        # –§–æ—Ä–º–∞—Ç: /homes-for-sale/{seoId}/{listingIdSHA}/
                        seo_id = listing['location']['seoId']
                        sha_id = listing.get('listingIdSHA') or listing_id
                        if sha_id:
                            url = f"{self.base_url}/homes-for-sale/{seo_id}/{sha_id}/"
                    
                    # –ü—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π URL
                    if not url and listing_id:
                        url = f"{self.base_url}/homes-for-sale/{listing_id}/"
                        
                    # –ü—Ä–æ–±—É–µ–º –ø–æ–ª–µ pageLink (–∏–∑ –ø—Ä–∏–º–µ—Ä–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
                    if not url and listing.get('pageLink'):
                        url = listing['pageLink']
                
                if url:
                    if not url.startswith('http'):
                        url = urljoin(self.base_url, url)
                    if url not in urls:
                        urls.append(url)
        
        # 2. –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –≤ HTML —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        if len(urls) < 5:
            logger.debug("–ò—â—É —Å—Å—ã–ª–∫–∏ –≤ HTML —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã...")
            
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–∞—Ä—Ç–æ—á–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            card_selectors = [
                'a[href*="/homes-for-sale/"]',
                'a[data-testid*="listing"]',
                'a[href*="/property/"]',
                '[class*="listing"] a',
                '[class*="property"] a',
                '[class*="card"] a',
            ]
            
            found_links = set()
            for selector in card_selectors:
                try:
                    elements = soup.select(selector)
                    for elem in elements:
                        href = elem.get('href', '')
                        if href and '/homes-for-sale/' in href:
                            found_links.add(href)
                except:
                    pass
            
            # –¢–∞–∫–∂–µ –∏—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º
            all_links = soup.find_all('a', href=True)
            known_locations = {
                'new-york', 'los-angeles', 'san-francisco', 'chicago', 
                'boston', 'miami', 'seattle', 'washington-dc', 'brooklyn',
                'manhattan', 'queens', 'bronx', 'staten-island', 'harlem',
                'upper-east-side', 'upper-west-side', 'west-village', 'east-village',
                'greenwich-village', 'soho', 'chelsea', 'flatiron', 'gramercy'
            }
            
            for link in all_links:
                href = link.get('href', '')
                if not href or href.startswith('#') or href.startswith('javascript:'):
                    continue
                
                # –ü–∞—Ç—Ç–µ—Ä–Ω URL: /homes-for-sale/{location}/{id}/ –∏–ª–∏ /homes-for-sale/{id}/
                match = re.search(r'/homes-for-sale/([^/?]+)/([^/?]+)/?', href)
                if match:
                    part1, part2 = match.groups()
                    # –ï—Å–ª–∏ –ø–µ—Ä–≤–∞—è —á–∞—Å—Ç—å - –∏–∑–≤–µ—Å—Ç–Ω–∞—è –ª–æ–∫–∞—Ü–∏—è, —Ç–æ ID - –≤—Ç–æ—Ä–∞—è —á–∞—Å—Ç—å
                    if part1.lower() in known_locations:
                        listing_id = part2
                    else:
                        # –ò–Ω–∞—á–µ –ø–µ—Ä–≤–∞—è —á–∞—Å—Ç—å –º–æ–∂–µ—Ç –±—ã—Ç—å ID
                        listing_id = part1
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ ID –æ–±—ä—è–≤–ª–µ–Ω–∏—è (–Ω–µ —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
                    # ID –æ–±—ä—è–≤–ª–µ–Ω–∏—è –æ–±—ã—á–Ω–æ –¥–ª–∏–Ω–Ω–µ–µ 5 —Å–∏–º–≤–æ–ª–æ–≤ –∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª—É–∂–µ–±–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º
                    is_valid_id = (
                        len(listing_id) > 5 and 
                        not listing_id.startswith(('start', 'page', 'sort', 'filter', 'search', 'price', 'bed')) and
                        (not listing_id.isdigit() or len(listing_id) > 10)  # –ï—Å–ª–∏ —Ü–∏—Ñ—Ä—ã, —Ç–æ –¥–ª–∏–Ω–Ω–µ–µ 10
                    )
                    if is_valid_id:
                        full_url = urljoin(self.base_url, href.split('?')[0])  # –£–±–∏—Ä–∞–µ–º query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                        found_links.add(full_url)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
            for link in found_links:
                if link not in urls:
                    urls.append(link)
            
            logger.debug(f"–ù–∞–π–¥–µ–Ω–æ {len(found_links)} —Å—Å—ã–ª–æ–∫ –≤ HTML")
                            
        return urls

    @staticmethod
    def extract_initial_data(html: str) -> dict[str, Any] | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç window.__INITIAL_DATA__ –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            soup = BeautifulSoup(html, 'lxml')
            scripts = soup.find_all('script')
            
            for script in scripts:
                script_text = script.string or script.get_text()
                if not script_text:
                    continue
                
                # –ò—â–µ–º window.__INITIAL_DATA__ = {...}
                # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω, —É—á–∏—Ç—ã–≤–∞—é—â–∏–π –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
                patterns = [
                    r'window\.__INITIAL_DATA__\s*=\s*({.+?});',
                    r'__INITIAL_DATA__\s*=\s*({.+?});',
                ]
                
                for pattern in patterns:
                    matches = list(re.finditer(pattern, script_text, re.DOTALL))
                    for match in matches:
                        json_str = match.group(1).strip()
                        try:
                            # –ü—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
                            data = json.loads(json_str)
                            if isinstance(data, dict) and len(data) > 0:
                                return data
                        except json.JSONDecodeError:
                            pass
            return None
        except Exception:
            return None

    @staticmethod
    def extract_listings_from_initial_data(data: dict[str, Any]) -> list[dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ __INITIAL_DATA__"""
        listings = []
        
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∫–ª—é—á–∞ 'listings' –∏–ª–∏ 'listing'
        def find_key(obj, key):
            if isinstance(obj, dict):
                if key in obj:
                    return obj[key]
                for k, v in obj.items():
                    res = find_key(v, key)
                    if res: return res
            elif isinstance(obj, list):
                for item in obj:
                    res = find_key(item, key)
                    if res: return res
            return None

        # –ò—â–µ–º —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        found = find_key(data, 'listings') or find_key(data, 'cards')
        if found and isinstance(found, list):
            listings = found
            
        return listings

    # ---------------------- –≠–¢–ê–ü 3: –ü–ê–†–°–ò–ù–ì –î–ê–ù–ù–´–• ----------------------

    def parse_listing_from_api_data(self, listing_data: dict) -> DbDTO | None:
        """
        –≠–¢–ê–ü 3 (API): –ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–∞–ø—Ä—è–º—É—é –∏–∑ API –æ—Ç–≤–µ—Ç–∞
        """
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∏ ID
            page_link = listing_data.get('pageLink') or listing_data.get('navigationPageLink', '')
            if not page_link:
                logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω pageLink –≤ –¥–∞–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
                return None
            
            if not page_link.startswith('http'):
                url = urljoin(self.base_url, page_link)
            else:
                url = page_link
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ URL
            listing_id = "unknown"
            parsed = urlparse(url)
            path_parts = [p for p in parsed.path.split('/') if p]
            if path_parts:
                listing_id = path_parts[-1]
            
            # –ú–∞–ø–ø–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –≤ DbDTO
            return self._map_to_dto_from_api(listing_data, url, listing_id)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –¥–∞–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
            import traceback
            traceback.print_exc()
            return None

    def parse_listing(self, url: str) -> DbDTO | None:
        """
        –≠–¢–ê–ü 3: –ü–∞—Ä—Å–∏—Ç HTML –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –∏–∑ window.__INITIAL_DATA__
        (–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ use_api=False)
        """
        html = self.get_page_source(url)
        if not html:
            return None
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ URL –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞
        listing_id = "unknown"
        parsed = urlparse(url)
        path_parts = [p for p in parsed.path.split('/') if p]
        if path_parts:
            listing_id = path_parts[-1]
        
        self._save_html_if_needed(html, listing_id)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        initial_data = self.extract_initial_data(html)
        if not initial_data:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å __INITIAL_DATA__ –¥–ª—è {url}")
            return None
        
        # –ò—â–µ–º –æ–±—ä–µ–∫—Ç –ª–∏—Å—Ç–∏–Ω–≥–∞ –≤–Ω—É—Ç—Ä–∏ –¥–∞–Ω–Ω—ã—Ö
        listing_data = self._find_listing_data(initial_data)
        if not listing_data:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –≤ JSON –¥–ª—è {url}")
            return None
            
        # –ú–∞–ø–ø–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –≤ DbDTO
        try:
            return self._map_to_dto(listing_data, url, listing_id)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –º–∞–ø–ø–∏–Ω–≥–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {url}: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _find_listing_data(self, data: dict) -> dict | None:
        """–ù–∞—Ö–æ–¥–∏—Ç –¥–∞–Ω–Ω—ã–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ"""
        # –û–±—ã—á–Ω–æ —ç—Ç–æ listingRelation -> listing –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ listing
        if 'listingRelation' in data and 'listing' in data['listingRelation']:
            return data['listingRelation']['listing']
        
        if 'listing' in data:
            return data['listing']
            
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
        def find_listing(obj):
            if isinstance(obj, dict):
                if 'listingIdSHA' in obj or 'compassPropertyId' in obj:
                    return obj
                for k, v in obj.items():
                    res = find_listing(v)
                    if res: return res
            return None
            
        return find_listing(data)

    def _map_to_dto(self, data: dict, url: str, listing_id: str) -> DbDTO:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç JSON –¥–∞–Ω–Ω—ã–µ –≤ DbDTO"""
        
        # Helper –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π
        def get_val(obj, path, default=None):
            for key in path.split('.'):
                if isinstance(obj, dict) and key in obj:
                    obj = obj[key]
                else:
                    return default
            return obj

        # Address
        location = data.get('location', {})
        address = location.get('prettyAddress') or get_val(data, 'address.prettyAddress') or "Address not found"
        
        # Price
        price_val = get_val(data, 'price.listed') or get_val(data, 'price.lastKnown')
        price_str = f"${price_val:,.0f}" if price_val else None
        
        # Type
        listing_type_code = data.get('listingType')
        listing_type = 'sale'  # Default
        if listing_type_code == 1: listing_type = 'lease' # –ü—Ä–∏–º–µ—Ä, –Ω—É–∂–Ω–æ —É—Ç–æ—á–Ω—è—Ç—å –∫–æ–¥—ã
        if 'rent' in str(data.get('detailedPropertyType', '')).lower():
            listing_type = 'lease'
            
        sale_price = price_str if listing_type == 'sale' else None
        lease_price = price_str if listing_type == 'lease' else None
        
        # Size
        size_sqft = get_val(data, 'size.squareFeet')
        size = f"{size_sqft:,.0f} SF" if size_sqft else None
        
        # Description
        description = data.get('description')
        
        # Photos
        photos = []
        media = data.get('media', [])
        for item in media:
            if item.get('type') == 0 and item.get('url'): # 0 = image
                photos.append(item['url'])
        
        # –ï—Å–ª–∏ —Ñ–æ—Ç–æ –Ω–µ –≤ media, –∏—â–µ–º –≤ –¥—Ä—É–≥–∏—Ö –º–µ—Å—Ç–∞—Ö
        if not photos:
            # Compass —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç gallery
            gallery = get_val(data, 'gallery', [])
            for item in gallery:
                if item.get('url'): photos.append(item['url'])

        # Agents
        agents = []
        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –∏–∑ listing.agents
        
        # Status
        status = data.get('localizedStatus') or "Available"
        
        # Details
        details = {
            'bedrooms': get_val(data, 'size.bedrooms'),
            'bathrooms': get_val(data, 'size.bathrooms'),
            'year_built': get_val(data, 'yearBuilt'),
            'property_type': get_val(data, 'propertyType.name'),
        }

        return DbDTO(
            source_name=self.source_name,
            listing_id=listing_id,
            listing_link=url,
            listing_type=listing_type,
            listing_status=status,
            address=address,
            sale_price=sale_price,
            lease_price=lease_price,
            size=size,
            property_description=description,
            listing_details=details,
            photos=photos,
            brochure_pdf=None,
            mls_number=None,
            agents=agents,
            agency_phone=None,
        )

    def _map_to_dto_from_api(self, listing_data: dict, url: str, listing_id: str) -> DbDTO:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ API –æ—Ç–≤–µ—Ç–∞ –≤ DbDTO"""
        
        # Helper –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π
        def get_val(obj, path, default=None):
            for key in path.split('.'):
                if isinstance(obj, dict) and key in obj:
                    obj = obj[key]
                else:
                    return default
            return obj
        
        # Address - –∏–∑ structuredData.singleFamilyResidence (JSON —Å—Ç—Ä–æ–∫–∞)
        address = "Address not found"
        structured_data = listing_data.get('structuredData', {})
        if 'singleFamilyResidence' in structured_data:
            try:
                sfr_str = structured_data['singleFamilyResidence']
                if isinstance(sfr_str, str):
                    sfr = json.loads(sfr_str)
                    if 'address' in sfr:
                        addr = sfr['address']
                        street = addr.get('streetAddress', '')
                        city = addr.get('addressLocality', '')
                        state = addr.get('addressRegion', '')
                        zip_code = addr.get('postalCode', '')
                        parts = [p for p in [street, city, state, zip_code] if p]
                        address = ', '.join(parts) if parts else "Address not found"
            except (json.JSONDecodeError, KeyError):
                pass
        
        # –ï—Å–ª–∏ –∞–¥—Ä–µ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º –∏–∑ –¥—Ä—É–≥–∏—Ö –º–µ—Å—Ç
        if address == "Address not found":
            location = listing_data.get('location', {})
            if isinstance(location, dict) and 'prettyAddress' in location:
                address = location['prettyAddress']
        
        # Price - –∏–∑ title (–Ω–∞–ø—Ä–∏–º–µ—Ä "$2,300,000")
        title = listing_data.get('title', '')
        sale_price = None
        lease_price = None
        listing_type = 'sale'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        if title and title.startswith('$'):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ –∏–∑ —Å—Ç—Ä–æ–∫–∏ —Ç–∏–ø–∞ "$2,300,000"
            price_match = re.search(r'\$([\d,]+)', title.replace(',', ''))
            if price_match:
                price_str = title  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–æ listingType –∏–ª–∏ –¥—Ä—É–≥–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
                listing_type_code = listing_data.get('status')  # status –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ —Ç–∏–ø
                # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è homes-for-sale —ç—Ç–æ –ø—Ä–æ–¥–∞–∂–∞
                sale_price = price_str
                listing_type = 'sale'
        
        # Size - –∏–∑ subStats
        size = None
        sub_stats = listing_data.get('subStats', [])
        for stat in sub_stats:
            if stat.get('title') == 'sqft':
                sqft_val = stat.get('subtitle', '').replace(',', '').replace('-', '').strip()
                if sqft_val and sqft_val != 'Unavailable':
                    try:
                        sqft_num = int(sqft_val)
                        size = f"{sqft_num:,} SF"
                    except ValueError:
                        size = sqft_val + " SF"
                break
        
        # Description - –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ structuredData –∏–ª–∏ –¥—Ä—É–≥–∏—Ö –º–µ—Å—Ç–∞—Ö
        description = None
        # TODO: –Ω–∞–π—Ç–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –≤ API –¥–∞–Ω–Ω—ã—Ö
        
        # Photos - –∏–∑ media
        photos = []
        media = listing_data.get('media', [])
        for item in media:
            # –í API media —Å–æ–¥–µ—Ä–∂–∏—Ç originalUrl –∏ thumbnailUrl
            if 'originalUrl' in item:
                photos.append(item['originalUrl'])
            elif 'thumbnailUrl' in item:
                photos.append(item['thumbnailUrl'])
            elif 'url' in item:
                photos.append(item['url'])
        
        # Status - –∏–∑ status (—á–∏—Å–ª–æ) –∏–ª–∏ –¥—Ä—É–≥–∏—Ö –ø–æ–ª–µ–π
        status = "Available"
        status_code = listing_data.get('status')
        if status_code:
            # –ú–∞–ø–ø–∏–Ω–≥ –∫–æ–¥–æ–≤ —Å—Ç–∞—Ç—É—Å–∞ (–Ω—É–∂–Ω–æ —É—Ç–æ—á–Ω–∏—Ç—å)
            status_map = {
                12: "Active",
                9: "Active",
                14: "Active",
                # –î–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –∫–æ–¥—ã –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            }
            status = status_map.get(status_code, f"Status {status_code}")
        
        # Details - –∏–∑ subStats –∏ –¥—Ä—É–≥–∏—Ö –ø–æ–ª–µ–π
        details = {}
        
        # Bedrooms, Bathrooms –∏–∑ subStats
        for stat in sub_stats:
            title_key = stat.get('title', '')
            subtitle = stat.get('subtitle', '').replace('-', '').strip()
            
            if title_key == 'beds' and subtitle and subtitle != 'Unavailable':
                try:
                    details['bedrooms'] = int(subtitle)
                except ValueError:
                    details['bedrooms'] = subtitle
            elif title_key == 'baths' and subtitle and subtitle != 'Unavailable':
                try:
                    details['bathrooms'] = float(subtitle) if '.' in subtitle else int(subtitle)
                except ValueError:
                    details['bathrooms'] = subtitle
            elif title_key == 'acres' and subtitle and subtitle != 'Unavailable':
                details['acres'] = subtitle
        
        # Property type –∏–∑ clusterSummary
        cluster_summary = listing_data.get('clusterSummary', {})
        if 'propertyType' in cluster_summary:
            prop_type = cluster_summary['propertyType']
            if isinstance(prop_type, dict) and 'masterType' in prop_type:
                master_type = prop_type['masterType']
                if isinstance(master_type, dict) and 'GLOBAL' in master_type:
                    types_list = master_type['GLOBAL']
                    if types_list:
                        details['property_type'] = types_list[0]
        
        # Agents - –ø–æ–∫–∞ –ø—É—Å—Ç–æ, –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤ API –¥–∞–Ω–Ω—ã—Ö
        agents = []
        
        # MLS number - –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤ –¥–∞–Ω–Ω—ã—Ö
        mls_number = None
        
        return DbDTO(
            source_name=self.source_name,
            listing_id=listing_id,
            listing_link=url,
            listing_type=listing_type,
            listing_status=status,
            address=address,
            sale_price=sale_price,
            lease_price=lease_price,
            size=size,
            property_description=description,
            listing_details=details if details else None,
            photos=photos if photos else None,
            brochure_pdf=None,
            mls_number=mls_number,
            agents=agents if agents else None,
            agency_phone=None,
        )

    def _save_html_if_needed(self, html: str, listing_id: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç HTML –≤ —Ñ–∞–π–ª"""
        self.html_counter += 1
        if self.html_counter % self.save_html_every == 0:
            safe_filename = re.sub(r'[^\w\-_\.]', '_', listing_id)
            if not safe_filename or safe_filename == '_':
                safe_filename = f"listing_{self.html_counter}"
            filepath = os.path.join(self.html_save_dir, f"{safe_filename}.html")
            try:
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(html)
                logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω HTML [{self.html_counter}]: {filepath}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ HTML: {e}")

    def run(self, location: str = "new-york", max_results: int = 1000, use_api: bool = True) -> list[DbDTO]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å"""
        try:
            if use_api:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º API - –ø–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞–ø—Ä—è–º—É—é, –±–µ–∑ Selenium
                logger.info("[API MODE] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö")
                listings_data = self.get_listings_from_api(location, max_results)
                
                if not listings_data:
                    logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ API")
                    return []
                
                # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ –∏–∑ API –æ—Ç–≤–µ—Ç–∞
                results = []
                logger.info(f"[2-3] –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(listings_data)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ API...")
                
                for i, listing_data in enumerate(listings_data):
                    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ [{i+1}/{len(listings_data)}]...")
                    dto = self.parse_listing_from_api_data(listing_data)
                    if dto:
                        results.append(dto)
                        logger.info(f"‚úì –£—Å–ø–µ—à–Ω–æ: {dto.address}")
                    else:
                        logger.warning(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏–µ {i+1}")
                
                logger.info(f"\n–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(results)}/{len(listings_data)}")
                return results
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º Selenium (—Å—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–±)
                logger.info("[SELENIUM MODE] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Selenium –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö")
                urls = self.get_listing_urls_from_search(location, max_results)
                if not urls:
                    logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
                    return []
                
                # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Selenium
                results = []
                logger.info(f"[2-3] –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(urls)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π...")
                
                for i, url in enumerate(urls):
                    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ [{i+1}/{len(urls)}]: {url}")
                    dto = self.parse_listing(url)
                    if dto:
                        results.append(dto)
                        logger.info(f"‚úì –£—Å–ø–µ—à–Ω–æ: {dto.address}")
                    
                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    time.sleep(1)
                    
                logger.info(f"\n–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(results)}/{len(urls)}")
                return results
            
        finally:
            if not use_api:  # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥—Ä–∞–π–≤–µ—Ä —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ Selenium
                self.stop_driver()

if __name__ == '__main__':
    parser = CompassParser(headless=False) # Headless=False –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    parser.run("new-york", 5)
