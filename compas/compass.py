import asyncio
import json
import logging
import os
import re
import uuid
from typing import Any
from urllib.parse import urljoin, urlparse

import httpx
from bs4 import BeautifulSoup
from fake_useragent import UserAgent

from schema import DbDTO, AgentData

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CompassParser:
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è compass.com —á–µ—Ä–µ–∑ API (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π)
    1. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ API (POST –∑–∞–ø—Ä–æ—Å—ã)
    2. –ü–æ–ª—É—á–µ–Ω–∏–µ HTML –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ª–∏—Å—Ç–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ httpx
    3. –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –∏–∑ API –∏ HTML –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
    """
    
    def __init__(
        self,
        save_html_every: int = 20,
        html_save_dir: str = "htmls",
        concurrency: int = 10,
    ) -> None:
        self.source_name = "compass"
        self.base_url = "https://www.compass.com"
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML
        self.save_html_every = save_html_every
        self.html_save_dir = html_save_dir
        self.html_counter = 0
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ concurrency
        self.concurrency = concurrency
        self.semaphore = asyncio.Semaphore(concurrency)
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        if not os.path.exists(self.html_save_dir):
            os.makedirs(self.html_save_dir)
            logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML: {self.html_save_dir}")

    # ---------------------- –≠–¢–ê–ü 1: –ò–ù–î–ï–ö–°–ê–¶–ò–Ø ----------------------

    def _split_area_into_grid(self, ne_point: dict, sw_point: dict, grid_size: int = 3) -> list[tuple]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –æ–±–ª–∞—Å—Ç—å –Ω–∞ –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ —á–∞—Å—Ç–∏ (grid) –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (ne_point, sw_point) –¥–ª—è –∫–∞–∂–¥–æ–π —è—á–µ–π–∫–∏ grid
        """
        ne_lat, ne_lng = ne_point['latitude'], ne_point['longitude']
        sw_lat, sw_lng = sw_point['latitude'], sw_point['longitude']
        
        lat_step = (ne_lat - sw_lat) / grid_size
        lng_step = (ne_lng - sw_lng) / grid_size
        
        grid_cells = []
        for i in range(grid_size):
            for j in range(grid_size):
                cell_ne_lat = ne_lat - (i * lat_step)
                cell_ne_lng = ne_lng - (j * lng_step)
                cell_sw_lat = ne_lat - ((i + 1) * lat_step)
                cell_sw_lng = ne_lng - ((j + 1) * lng_step)
                
                cell_ne = {"latitude": cell_ne_lat, "longitude": cell_ne_lng}
                cell_sw = {"latitude": cell_sw_lat, "longitude": cell_sw_lng}
                
                grid_cells.append((cell_ne, cell_sw))
        
        return grid_cells

    async def get_listings_from_api(self, client: httpx.AsyncClient, location: str = "new-york", max_results: int = 1000, use_grid: bool = True) -> list[dict]:
        """
        –≠–¢–ê–ü 1 (API): –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ API compass.com (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–≤–∫–ª—é—á–∞—è listing –æ–±—ä–µ–∫—Ç—ã)
        
        Args:
            use_grid: –ï—Å–ª–∏ True, —Ä–∞–∑–±–∏–≤–∞–µ—Ç –æ–±–ª–∞—Å—Ç—å –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        """
        logger.info(f"[1-API] –ü–æ–ª—É—á–∞—é –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ API –¥–ª—è –ª–æ–∫–∞—Ü–∏–∏: {location}")
        
        all_listings_data = []
        search_result_id = str(uuid.uuid4())
        
        # –ë–∞–∑–æ–≤—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è New York (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –¥–ª—è –¥—Ä—É–≥–∏—Ö –ª–æ–∫–∞—Ü–∏–π)
        # –≠—Ç–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ø–æ–∫—Ä—ã–≤–∞—é—Ç –≤–µ—Å—å —à—Ç–∞—Ç NY
        ne_point = {"latitude": 45.3525295, "longitude": -72.3285732}
        sw_point = {"latitude": 39.9017281, "longitude": -79.2115078}
        viewport_ne = {"lat": 45.2954092, "lng": -72.3285732}
        viewport_sw = {"lat": 39.839376, "lng": -79.2115078}
        
        if use_grid:
            # –†–∞–∑–±–∏–≤–∞–µ–º –æ–±–ª–∞—Å—Ç—å –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            logger.info(f"[1-API] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è grid-–ø–æ–¥—Ö–æ–¥: —Ä–∞–∑–±–∏–≤–∞–µ–º –æ–±–ª–∞—Å—Ç—å –Ω–∞ —á–∞—Å—Ç–∏")
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º grid_size –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            grid_size = 6  # 6x6 = 36 —á–∞—Å—Ç–µ–π (–±–æ–ª—å—à–µ –ø–æ–∫—Ä—ã—Ç–∏–µ)
            grid_cells = self._split_area_into_grid(ne_point, sw_point, grid_size=grid_size)
            logger.info(f"[1-API] –û–±–ª–∞—Å—Ç—å —Ä–∞–∑–±–∏—Ç–∞ –Ω–∞ {len(grid_cells)} —á–∞—Å—Ç–µ–π ({grid_size}x{grid_size})")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            tasks = []
            for idx, (cell_ne, cell_sw) in enumerate(grid_cells):
                cell_viewport_ne = {"lat": cell_ne["latitude"], "lng": cell_ne["longitude"]}
                cell_viewport_sw = {"lat": cell_sw["latitude"], "lng": cell_sw["longitude"]}
                task = self._get_listings_for_area(
                    client, location, str(uuid.uuid4()),  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π search_result_id –¥–ª—è –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏
                    cell_ne, cell_sw, cell_viewport_ne, cell_viewport_sw,
                    max_results // len(grid_cells) + 50  # –ù–µ–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ –Ω–∞ —á–∞—Å—Ç—å
                )
                tasks.append(task)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for idx, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"[1-API] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —á–∞—Å—Ç–∏ {idx + 1}: {result}")
                else:
                    all_listings_data.extend(result)
                    logger.info(f"[1-API] –ß–∞—Å—Ç—å {idx + 1}/{len(grid_cells)}: –ø–æ–ª—É—á–µ–Ω–æ {len(result)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ listingIdSHA
            seen_ids = set()
            unique_listings = []
            for listing in all_listings_data:
                listing_id = listing.get('listingIdSHA')
                if listing_id and listing_id not in seen_ids:
                    seen_ids.add(listing_id)
                    unique_listings.append(listing)
            
            logger.info(f"[1-API] –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(unique_listings)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ {len(all_listings_data)}")
            return unique_listings[:max_results]
        else:
            # –°—Ç–∞—Ä—ã–π –ø–æ–¥—Ö–æ–¥ - –æ–¥–Ω–∞ –æ–±–ª–∞—Å—Ç—å
            return await self._get_listings_for_area(
                client, location, search_result_id,
                ne_point, sw_point, viewport_ne, viewport_sw,
                max_results
            )

    async def _get_listings_for_area(
        self, 
        client: httpx.AsyncClient,
        location: str,
        search_result_id: str,
        ne_point: dict,
        sw_point: dict,
        viewport_ne: dict,
        viewport_sw: dict,
        max_results: int
    ) -> list[dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏"""
        listings_data = []
        location_ids = None
        page = 0
        num_per_page = 50
        
        try:
            while len(listings_data) < max_results:
                headers = {
                    'User-Agent': UserAgent().random,
                    'Accept': '*/*',
                    'Accept-Language': 'en-US,en;q=0.5',
                    'Referer': f'https://www.compass.com/homes-for-sale/{location}/',
                    'Content-Type': 'application/json',
                    'Origin': 'https://www.compass.com',
                    'Sec-GPC': '1',
                    'Sec-Fetch-Dest': 'empty',
                    'Sec-Fetch-Mode': 'cors',
                    'Sec-Fetch-Site': 'same-origin',
                    'Priority': 'u=6',
                }
                
                # –ü–∞–≥–∏–Ω–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä start –≤ searchQuery URL!
                search_query = {
                    "sort": {"column": "dom", "direction": "asc"},
                    "start": page * num_per_page  # –î–æ–±–∞–≤–ª—è–µ–º start –¥–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
                }
                params = {
                    'searchQuery': json.dumps(search_query),
                }
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º rawLolSearchQuery
                raw_query = {
                    'listingTypes': [2],  # 2 = For Sale
                    'saleStatuses': [12, 9],  # Active listings
                    'num': num_per_page,
                    'start': page * num_per_page,
                    'sortOrder': 46,  # DOM ascending
                    'facetFieldNames': [
                        'contributingDatasetList',
                        'compassListingTypes',
                        'comingSoon',
                    ],
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º locationIds –µ—Å–ª–∏ –µ—Å—Ç—å (–∏–∑ –ø–µ—Ä–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞)
                if location_ids:
                    raw_query['locationIds'] = location_ids
                else:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                    raw_query['nePoint'] = ne_point
                    raw_query['swPoint'] = sw_point
                
                json_data = {
                    'searchResultId': search_result_id,
                    'rawLolSearchQuery': raw_query,
                    'viewport': {
                        'northeast': viewport_ne,
                        'southwest': viewport_sw,
                    },
                    'viewportFrom': 'response',
                    'height': 1350,
                    'width': 1253,
                    'isMapFullyInitialized': True,
                    'purpose': 'search',
                }
                
                api_url = f"{self.base_url}/homes-for-sale/{location}/mapview={viewport_ne['lat']},{viewport_ne['lng']},{viewport_sw['lat']},{viewport_sw['lng']}/"
                
                logger.info(f"[1-API] –ó–∞–ø—Ä–æ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page + 1}, start={page * num_per_page}")
                
                try:
                    async with self.semaphore:
                        response = await client.post(
                            api_url,
                            params=params,
                            json=json_data,
                            headers=headers,
                            timeout=30.0
                        )
                        response.raise_for_status()
                    
                    data = response.json()
                    
                    if 'lolResults' not in data or 'data' not in data['lolResults']:
                        logger.warning(f"[1-API] –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ API")
                        break
                    
                    listings = data['lolResults']['data']
                    total_items = data['lolResults'].get('totalItems', 0)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º locationIds –∏–∑ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                    if not location_ids and 'rawLolSearchQuery' in data:
                        response_query = data.get('rawLolSearchQuery', {})
                        if 'locationIds' in response_query:
                            location_ids = response_query['locationIds']
                            logger.info(f"[1-API] –ù–∞–π–¥–µ–Ω—ã locationIds: {location_ids}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
                    
                    logger.info(f"[1-API] –ü–æ–ª—É—á–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–≤—Å–µ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ: {total_items})")
                    
                    if not listings:
                        logger.info(f"[1-API] –ë–æ–ª—å—à–µ –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –ó–∞–≤–µ—Ä—à–∞–µ–º.")
                        break
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–≤–∫–ª—é—á–∞—è listing –æ–±—ä–µ–∫—Ç—ã)
                    for item in listings:
                        listing = item.get('listing', {})
                        if listing:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ listing
                            listings_data.append(listing)
                    
                    logger.info(f"[1-API] –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page + 1} –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –í—Å–µ–≥–æ: {len(listings_data)}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ –º—ã –ª–∏–º–∏—Ç–∞
                    if len(listings_data) >= max_results:
                        logger.info(f"[1-API] –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç max_results ({max_results}). –ó–∞–≤–µ—Ä—à–∞–µ–º.")
                        break
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–ª—É—á–∏–ª–∏ –ª–∏ –º—ã –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
                    if total_items > 0 and len(listings_data) >= total_items:
                        logger.info(f"[1-API] –ü–æ–ª—É—á–µ–Ω—ã –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è ({total_items}). –ó–∞–≤–µ—Ä—à–∞–µ–º.")
                        break
                    
                    # –ü–∞–≥–∏–Ω–∞—Ü–∏—è —Ç–µ–ø–µ—Ä—å —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ —á–µ—Ä–µ–∑ start –≤ searchQuery URL!
                    # –ù–µ –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏, —Ç–∞–∫ –∫–∞–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–Ω—ã–µ
                    
                    # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –º–µ–Ω—å—à–µ, —á–µ–º –∑–∞–ø—Ä–∞—à–∏–≤–∞–ª–∏, –ø—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –µ—â–µ –¥–∞–Ω–Ω—ã–µ
                    if len(listings) < num_per_page:
                        # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –º–µ–Ω—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –µ—â–µ –µ—Å—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏—è - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                        if total_items > 0 and len(listings_data) < total_items:
                            remaining = total_items - len(listings_data)
                            logger.info(f"[1-API] –ü–æ–ª—É—á–µ–Ω–æ –º–µ–Ω—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ({len(listings)}), –Ω–æ –µ—â–µ –µ—Å—Ç—å {remaining} –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º.")
                        else:
                            # –ï—Å–ª–∏ total_items –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω –∏–ª–∏ –º—ã –ø–æ–ª—É—á–∏–ª–∏ –≤—Å–µ - –∑–∞–≤–µ—Ä—à–∞–µ–º
                            logger.info(f"[1-API] –ü–æ–ª—É—á–µ–Ω–æ –º–µ–Ω—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –±–æ–ª—å—à–µ –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –ó–∞–≤–µ—Ä—à–∞–µ–º.")
                            break
                    # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ - –∑–∞–≤–µ—Ä—à–∞–µ–º
                    elif len(listings) == 0:
                        logger.info(f"[1-API] –ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –ó–∞–≤–µ—Ä—à–∞–µ–º.")
                        break
                    
                    page += 1
                    
                    # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    await asyncio.sleep(0.5)
                    
                except httpx.HTTPStatusError as e:
                    logger.error(f"[1-API] HTTP –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API: {e}")
                    break
                except httpx.RequestError as e:
                    logger.error(f"[1-API] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API: {e}")
                    break
                except json.JSONDecodeError as e:
                    logger.error(f"[1-API] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ JSON –æ—Ç–≤–µ—Ç–∞: {e}")
                    break
            
            logger.info(f"[1-API] –ò—Ç–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ {len(listings_data)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            return listings_data[:max_results]
            
        except Exception as e:
            logger.error(f"[1-API] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ API: {e}")
            import traceback
            traceback.print_exc()
            return listings_data


    # ---------------------- –≠–¢–ê–ü 3: –ü–ê–†–°–ò–ù–ì –î–ê–ù–ù–´–• ----------------------

    async def get_listing_html(self, client: httpx.AsyncClient, url: str) -> str | None:
        """–ü–æ–ª—É—á–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—ä—è–≤–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ httpx (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)"""
        try:
            headers = {
                'User-Agent': UserAgent().random,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Referer': 'https://www.compass.com/',
            }
            async with self.semaphore:
                response = await client.get(url, headers=headers, timeout=15.0, follow_redirects=True)
                response.raise_for_status()
                return response.text
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å HTML –¥–ª—è {url}: {e}")
            return None

    async def parse_listing_from_api_data(self, client: httpx.AsyncClient, listing_data: dict) -> DbDTO | None:
        """
        –≠–¢–ê–ü 3 (API): –ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–∞–ø—Ä—è–º—É—é –∏–∑ API –æ—Ç–≤–µ—Ç–∞ (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)
        –î–ª—è –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –ø–æ–ª–µ–π (–æ–ø–∏—Å–∞–Ω–∏–µ, MLS, brochure, –∞–≥–µ–Ω—Ç—ã) –∑–∞–≥—Ä—É–∂–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        """
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∏ ID
            page_link = listing_data.get('pageLink') or listing_data.get('navigationPageLink', '')
            if not page_link:
                logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω pageLink –≤ –¥–∞–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
                return None
            
            if not page_link.startswith('http'):
                url = urljoin(self.base_url, page_link)
            else:
                url = page_link
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ URL
            listing_id = "unknown"
            parsed = urlparse(url)
            path_parts = [p for p in parsed.path.split('/') if p]
            if path_parts:
                listing_id = path_parts[-1]
            
            # –°–Ω–∞—á–∞–ª–∞ –º–∞–ø–ø–∏–º –¥–∞–Ω–Ω—ã–µ –∏–∑ API
            dto = self._map_to_dto_from_api(listing_data, url, listing_id)
            
            # –ó–∞—Ç–µ–º –∑–∞–≥—Ä—É–∂–∞–µ–º HTML –¥–ª—è –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –ø–æ–ª–µ–π
            html = await self.get_listing_html(client, url)
            if html:
                # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ HTML
                self._enrich_from_html(dto, html, listing_id)
            
            return dto
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –¥–∞–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
            import traceback
            traceback.print_exc()
            return None


    def _map_to_dto_from_api(self, listing_data: dict, url: str, listing_id: str) -> DbDTO:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ API –æ—Ç–≤–µ—Ç–∞ –≤ DbDTO"""
        
        # Helper –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π
        def get_val(obj, path, default=None):
            for key in path.split('.'):
                if isinstance(obj, dict) and key in obj:
                    obj = obj[key]
                else:
                    return default
            return obj
        
        # Address - –∏–∑ structuredData.singleFamilyResidence (JSON —Å—Ç—Ä–æ–∫–∞)
        address = "Address not found"
        structured_data = listing_data.get('structuredData', {})
        if 'singleFamilyResidence' in structured_data:
            try:
                sfr_str = structured_data['singleFamilyResidence']
                if isinstance(sfr_str, str):
                    sfr = json.loads(sfr_str)
                    if 'address' in sfr:
                        addr = sfr['address']
                        street = addr.get('streetAddress', '')
                        city = addr.get('addressLocality', '')
                        state = addr.get('addressRegion', '')
                        zip_code = addr.get('postalCode', '')
                        parts = [p for p in [street, city, state, zip_code] if p]
                        address = ', '.join(parts) if parts else "Address not found"
            except (json.JSONDecodeError, KeyError):
                pass
        
        # –ï—Å–ª–∏ –∞–¥—Ä–µ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º –∏–∑ –¥—Ä—É–≥–∏—Ö –º–µ—Å—Ç
        if address == "Address not found":
            location = listing_data.get('location', {})
            if isinstance(location, dict) and 'prettyAddress' in location:
                address = location['prettyAddress']
        
        # Price - –∏–∑ title (–Ω–∞–ø—Ä–∏–º–µ—Ä "$2,300,000")
        title = listing_data.get('title', '')
        sale_price = None
        lease_price = None
        listing_type = 'sale'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        if title and title.startswith('$'):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ –∏–∑ —Å—Ç—Ä–æ–∫–∏ —Ç–∏–ø–∞ "$2,300,000"
            price_match = re.search(r'\$([\d,]+)', title.replace(',', ''))
            if price_match:
                price_str = title  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–æ listingType –∏–ª–∏ –¥—Ä—É–≥–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
                listing_type_code = listing_data.get('status')  # status –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ —Ç–∏–ø
                # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è homes-for-sale —ç—Ç–æ –ø—Ä–æ–¥–∞–∂–∞
                sale_price = price_str
                listing_type = 'sale'
        
        # Size - –∏–∑ subStats (–ø–ª–æ—â–∞–¥—å)
        size = None
        sub_stats = listing_data.get('subStats', [])
        for stat in sub_stats:
            title_key = stat.get('title', '')
            subtitle = stat.get('subtitle', '').replace(',', '').replace('-', '').strip()
            
            if title_key == 'sqft' and subtitle and subtitle != 'Unavailable':
                try:
                    sqft_num = int(subtitle)
                    size = f"{sqft_num:,} SF"
                except ValueError:
                    size = subtitle + " SF"
                break
            elif title_key == 'acres' and subtitle and subtitle != 'Unavailable' and not size:
                # –ï—Å–ª–∏ –Ω–µ—Ç sqft, –∏—Å–ø–æ–ª—å–∑—É–µ–º acres
                size = f"{subtitle} acres"
        
        # Description - –∏–∑ structuredData.product (–∫–æ—Ä–æ—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ)
        # –ü–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ HTML
        description = None
        structured_data = listing_data.get('structuredData', {})
        if 'product' in structured_data:
            try:
                product_str = structured_data['product']
                if isinstance(product_str, str):
                    product = json.loads(product_str)
                    description = product.get('description', '')
            except (json.JSONDecodeError, KeyError):
                pass
        
        # Photos - –∏–∑ media
        photos = []
        media = listing_data.get('media', [])
        for item in media:
            # –í API media —Å–æ–¥–µ—Ä–∂–∏—Ç originalUrl –∏ thumbnailUrl
            if 'originalUrl' in item:
                photos.append(item['originalUrl'])
            elif 'thumbnailUrl' in item:
                photos.append(item['thumbnailUrl'])
            elif 'url' in item:
                photos.append(item['url'])
        
        # Status - –∏–∑ badges –∏–ª–∏ status
        status = "Available"
        badges = listing_data.get('badges', {})
        corner_badges = badges.get('cornerBadges', [])
        if corner_badges:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π badge (–æ–±—ã—á–Ω–æ —ç—Ç–æ —Å—Ç–∞—Ç—É—Å —Ç–∏–ø–∞ "Coming Soon")
            status = corner_badges[0].get('displayText', 'Available')
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç badges, –∏—Å–ø–æ–ª—å–∑—É–µ–º status –∫–æ–¥
            status_code = listing_data.get('status')
            if status_code:
                status_map = {
                    12: "Active",
                    9: "Active",
                    14: "Active",
                    # –î–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –∫–æ–¥—ã –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                }
                status = status_map.get(status_code, f"Status {status_code}")
        
        # Details - –∏–∑ subStats –∏ –¥—Ä—É–≥–∏—Ö –ø–æ–ª–µ–π (extract_details)
        details = {}
        
        # Bedrooms, Bathrooms –∏–∑ subStats
        for stat in sub_stats:
            title_key = stat.get('title', '')
            subtitle = stat.get('subtitle', '').replace('-', '').strip()
            
            if title_key == 'beds' and subtitle and subtitle != 'Unavailable':
                try:
                    details['bedrooms'] = int(subtitle)
                except ValueError:
                    details['bedrooms'] = subtitle
            elif title_key == 'baths' and subtitle and subtitle != 'Unavailable':
                try:
                    details['bathrooms'] = float(subtitle) if '.' in subtitle else int(subtitle)
                except ValueError:
                    details['bathrooms'] = subtitle
            elif title_key == 'acres' and subtitle and subtitle != 'Unavailable':
                details['acres'] = subtitle
            elif title_key == 'sqft' and subtitle and subtitle != 'Unavailable':
                try:
                    details['square_feet'] = int(subtitle.replace(',', ''))
                except ValueError:
                    details['square_feet'] = subtitle
        
        # Property type –∏–∑ clusterSummary
        cluster_summary = listing_data.get('clusterSummary', {})
        if 'propertyType' in cluster_summary:
            prop_type = cluster_summary['propertyType']
            if isinstance(prop_type, dict) and 'masterType' in prop_type:
                master_type = prop_type['masterType']
                if isinstance(master_type, dict) and 'GLOBAL' in master_type:
                    types_list = master_type['GLOBAL']
                    if types_list:
                        details['property_type'] = types_list[0]
        
        # Price range –∏–∑ clusterSummary
        if 'priceRange' in cluster_summary:
            price_range = cluster_summary['priceRange']
            if isinstance(price_range, list) and price_range:
                details['price_range'] = price_range
        
        # Agents - –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –∏–∑ HTML
        agents = []
        
        # MLS number - –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –∏–∑ HTML (extract_mls)
        mls_number = None
        
        return DbDTO(
            source_name=self.source_name,
            listing_id=listing_id,
            listing_link=url,
            listing_type=listing_type,
            listing_status=status,
            address=address,
            sale_price=sale_price,
            lease_price=lease_price,
            size=size,
            property_description=description,
            listing_details=details if details else None,
            photos=photos if photos else None,
            brochure_pdf=None,
            mls_number=mls_number,
            agents=agents if agents else None,
            agency_phone=None,
        )

    def _enrich_from_html(self, dto: DbDTO, html: str, listing_id: str) -> None:
        """–î–æ–ø–æ–ª–Ω—è–µ—Ç DTO –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–æ–ø–∏—Å–∞–Ω–∏–µ, MLS, brochure, –∞–≥–µ–Ω—Ç—ã)"""
        try:
            soup = BeautifulSoup(html, 'lxml')
            
            # 1. Extract MLS number
            mls = self.extract_mls(soup)
            if mls:
                dto.mls_number = mls
            
            # 2. Extract description
            description = self.extract_description(soup)
            if description and not dto.property_description:
                dto.property_description = description
            
            # 3. Extract brochure PDF
            brochure = self.extract_brochure_pdf(soup)
            if brochure:
                dto.brochure_pdf = brochure
            
            # 4. Extract agents
            agents = self.extract_agents(soup, self.base_url)
            if agents:
                dto.agents = agents
            
            # 5. Extract additional details
            additional_details = self.extract_details(soup)
            if additional_details:
                if dto.listing_details:
                    dto.listing_details.update(additional_details)
                else:
                    dto.listing_details = additional_details
            
            # 6. Extract size –µ—Å–ª–∏ –Ω–µ –±—ã–ª–æ –Ω–∞–π–¥–µ–Ω–æ
            if not dto.size:
                size = self.extract_size(soup)
                if size:
                    dto.size = size
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ HTML: {e}")

    @staticmethod
    def extract_mls(soup: BeautifulSoup) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç MLS –Ω–æ–º–µ—Ä –∏–∑ HTML"""
        mls_patterns = [
            re.compile(r'MLS[#:\s]*([A-Z0-9\-]+)', re.I),
            re.compile(r'MLS\s*Number[#:\s]*([A-Z0-9\-]+)', re.I),
            re.compile(r'Multiple\s*Listing\s*Service[#:\s]*([A-Z0-9\-]+)', re.I),
        ]
        
        page_text = soup.get_text()
        for pattern in mls_patterns:
            match = pattern.search(page_text)
            if match:
                return match.group(1).strip()
        
        # –ò—â–µ–º –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        scripts = soup.find_all('script', type='application/ld+json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, dict):
                    # –ò—â–µ–º MLS –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
                    if 'identifier' in data:
                        identifier = data['identifier']
                        if isinstance(identifier, dict) and identifier.get('@type') == 'PropertyValue':
                            value = identifier.get('value', '')
                            if 'MLS' in value.upper():
                                return value
            except (json.JSONDecodeError, AttributeError):
                continue
        
        return None

    @staticmethod
    def extract_description(soup: BeautifulSoup) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ HTML"""
        # –ò—â–µ–º –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
        selectors = [
            'div[class*="description"]',
            'div[class*="property-description"]',
            'div[class*="listing-description"]',
            'section[class*="description"]',
            '[data-testid*="description"]',
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text(strip=True)
                if text and len(text) > 50:  # –ú–∏–Ω–∏–º—É–º 50 —Å–∏–º–≤–æ–ª–æ–≤
                    return text
        
        # –ò—â–µ–º –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        scripts = soup.find_all('script', type='application/ld+json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, dict):
                    if 'description' in data:
                        desc = data['description']
                        if isinstance(desc, str) and len(desc) > 50:
                            return desc
            except (json.JSONDecodeError, AttributeError):
                continue
        
        return None

    @staticmethod
    def extract_brochure_pdf(soup: BeautifulSoup) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ brochure PDF"""
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ PDF
        pdf_links = soup.find_all('a', href=re.compile(r'\.pdf$', re.I))
        for link in pdf_links:
            href = link.get('href', '')
            text = link.get_text(strip=True).lower()
            if 'brochure' in text or 'flyer' in text or 'marketing' in text:
                if not href.startswith('http'):
                    href = urljoin('https://www.compass.com', href)
                return href
        
        # –ò—â–µ–º –≤ data-–∞—Ç—Ä–∏–±—É—Ç–∞—Ö
        elements = soup.find_all(attrs={'data-brochure': True})
        for elem in elements:
            brochure_url = elem.get('data-brochure')
            if brochure_url:
                if not brochure_url.startswith('http'):
                    brochure_url = urljoin('https://www.compass.com', brochure_url)
                return brochure_url
        
        return None

    @staticmethod
    def extract_agents(soup: BeautifulSoup, base_url: str) -> list[AgentData]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–≥–µ–Ω—Ç–æ–≤ –∏–∑ HTML"""
        agents = []
        
        # –ò—â–µ–º –±–ª–æ–∫–∏ —Å –∞–≥–µ–Ω—Ç–∞–º–∏
        agent_selectors = [
            '[class*="agent"]',
            '[class*="listing-agent"]',
            '[data-testid*="agent"]',
            '[class*="broker"]',
        ]
        
        for selector in agent_selectors:
            elements = soup.select(selector)
            for elem in elements:
                # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –±–ª–æ–∫–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
                full_text = elem.get_text(separator=' ', strip=True)
                
                # –ò—â–µ–º –∏–º—è - —Å–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ—Ç–¥–µ–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç
                name = None
                name_elem = elem.find(['h3', 'h4', 'h5', 'div', 'span'], class_=re.compile(r'name|agent', re.I))
                if not name_elem:
                    name_elem = elem.find('a', href=re.compile(r'/agent/|/team/'))
                
                if name_elem:
                    name = name_elem.get_text(strip=True)
                else:
                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –æ—Ç–¥–µ–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç, –ø–∞—Ä—Å–∏–º –∏–∑ —Ç–µ–∫—Å—Ç–∞
                    # –§–æ—Ä–º–∞—Ç: "Listed byLynn Wadleigh ‚Ä¢ Coldwell Banker..."
                    name_match = re.search(r'(?:Listed\s+by|Agent:?)\s*([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', full_text)
                    if name_match:
                        name = name_match.group(1).strip()
                
                # –ï—Å–ª–∏ –∏–º—è –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                if not name or len(name) < 2:
                    continue
                
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –ø—Ä–æ—Ñ–∏–ª—å –∞–≥–µ–Ω—Ç–∞
                link = None
                a_tag = elem.find('a', href=re.compile(r'/agent/|/team/'))
                if a_tag:
                    link = a_tag.get('href', '')
                    if link and not link.startswith('http'):
                        link = urljoin(base_url, link)
                
                # –ü–∞—Ä—Å–∏–º —Ç–µ–ª–µ—Ñ–æ–Ω –∏–∑ —Ç–µ–∫—Å—Ç–∞
                # –§–æ—Ä–º–∞—Ç—ã: P:(518)-260-2222, C:(518)-260-2222, Phone: (518) 260-2222
                phone_primary = None
                phone_alt = None
                
                # –ò—â–µ–º —á–µ—Ä–µ–∑ tel: —Å—Å—ã–ª–∫–∏
                phone_elems = elem.find_all('a', href=re.compile(r'tel:'))
                for phone_elem in phone_elems:
                    phone_match = re.search(r'tel:([\d\s\-\(\)]+)', phone_elem.get('href', ''))
                    if phone_match:
                        if not phone_primary:
                            phone_primary = phone_match.group(1).strip()
                        else:
                            phone_alt = phone_match.group(1).strip()
                
                # –ü–∞—Ä—Å–∏–º –∏–∑ —Ç–µ–∫—Å—Ç–∞: P:(518)-260-2222 –∏–ª–∏ C:(518)-260-2222
                if not phone_primary:
                    phone_patterns = [
                        r'P:\s*\(?(\d{3})\)?\s*-?\s*(\d{3})\s*-?\s*(\d{4})',  # P:(518)-260-2222
                        r'Phone:\s*\(?(\d{3})\)?\s*-?\s*(\d{3})\s*-?\s*(\d{4})',  # Phone: (518) 260-2222
                        r'\((\d{3})\)\s*(\d{3})\s*-?\s*(\d{4})',  # (518) 260-2222
                    ]
                    for pattern in phone_patterns:
                        match = re.search(pattern, full_text)
                        if match:
                            phone_primary = f"({match.group(1)}) {match.group(2)}-{match.group(3)}"
                            break
                
                # –ò—â–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω (C:)
                if not phone_alt:
                    cell_match = re.search(r'C:\s*\(?(\d{3})\)?\s*-?\s*(\d{3})\s*-?\s*(\d{4})', full_text)
                    if cell_match:
                        phone_alt = f"({cell_match.group(1)}) {cell_match.group(2)}-{cell_match.group(3)}"
                
                # –ü–∞—Ä—Å–∏–º email
                email = None
                email_elem = elem.find('a', href=re.compile(r'mailto:'))
                if email_elem:
                    email_match = re.search(r'mailto:([^\s"\'<>]+)', email_elem.get('href', ''))
                    if email_match:
                        email = email_match.group(1).strip()
                else:
                    # –ü–∞—Ä—Å–∏–º email –∏–∑ —Ç–µ–∫—Å—Ç–∞
                    email_match = re.search(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', full_text)
                    if email_match:
                        email = email_match.group(1).strip()
                
                # –ü–∞—Ä—Å–∏–º –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ñ–∏—Å–∞
                # –§–æ—Ä–º–∞—Ç: "‚Ä¢ Coldwell Banker Prime Properties"
                office_name = None
                office_match = re.search(r'‚Ä¢\s*([A-Z][^‚Ä¢P:C:@]+?)(?:\s+P:|C:|@|$)', full_text)
                if office_match:
                    office_name = office_match.group(1).strip()
                else:
                    # –ò—â–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —ç–ª–µ–º–µ–Ω—Ç–µ
                    office_elem = elem.find(['div', 'span'], class_=re.compile(r'office|brokerage|company', re.I))
                    if office_elem:
                        office_name = office_elem.get_text(strip=True)
                
                # –ò—â–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω –æ—Ñ–∏—Å–∞
                office_phone = None
                office_phone_elem = elem.find(['div', 'span'], class_=re.compile(r'office.*phone|brokerage.*phone', re.I))
                if office_phone_elem:
                    office_phone_match = re.search(r'\(?(\d{3})\)?\s*-?\s*(\d{3})\s*-?\s*(\d{4})', office_phone_elem.get_text())
                    if office_phone_match:
                        office_phone = f"({office_phone_match.group(1)}) {office_phone_match.group(2)}-{office_phone_match.group(3)}"
                
                # –ò—â–µ–º —Ñ–æ—Ç–æ
                photo_url = None
                img = elem.find('img')
                if img:
                    photo_url = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
                    if photo_url and not photo_url.startswith('http'):
                        photo_url = urljoin(base_url, photo_url)
                
                # –ò—â–µ–º –¥–æ–ª–∂–Ω–æ—Å—Ç—å
                title = None
                title_elem = elem.find(['div', 'span'], class_=re.compile(r'title|position|role', re.I))
                if title_elem:
                    title = title_elem.get_text(strip=True)
                
                # –û—á–∏—â–∞–µ–º –∏–º—è –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤
                if name:
                    name = re.sub(r'^Listed\s+by\s*', '', name, flags=re.I).strip()
                    name = re.sub(r'\s*‚Ä¢.*$', '', name).strip()  # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–æ—Å–ª–µ ‚Ä¢
                
                agent = AgentData(
                    name=name,
                    title=title,
                    phone_primary=phone_primary,
                    phone_alt=phone_alt,
                    email=email,
                    photo_url=photo_url,
                    social_media=link,
                    office_name=office_name,
                    office_phone=office_phone,
                )
                agents.append(agent)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∏–º–µ–Ω–∏ –∏ email
        seen = set()
        unique_agents = []
        for agent in agents:
            key = (agent.name, agent.email)
            if key not in seen and agent.name:
                seen.add(key)
                unique_agents.append(agent)
        
        return unique_agents

    @staticmethod
    def extract_details(soup: BeautifulSoup) -> dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è (—Ç–∞–±–ª–∏—Ü–∞) –∏–∑ HTML"""
        details = {}
        
        # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—ã —Å –¥–µ—Ç–∞–ª—è–º–∏
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    key = cells[0].get_text(strip=True)
                    value = cells[1].get_text(strip=True)
                    if key and value:
                        details[key.lower().replace(' ', '_')] = value
        
        # –ò—â–µ–º —Å–ø–∏—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π (dl)
        dl_elements = soup.find_all('dl')
        for dl in dl_elements:
            dts = dl.find_all('dt')
            dds = dl.find_all('dd')
            for dt, dd in zip(dts, dds):
                key = dt.get_text(strip=True)
                value = dd.get_text(strip=True)
                if key and value:
                    details[key.lower().replace(' ', '_')] = value
        
        # –ò—â–µ–º div —Å –ø–∞—Ä–∞–º–∏ –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ
        detail_divs = soup.find_all(['div', 'section'], class_=re.compile(r'detail|feature|spec', re.I))
        for div in detail_divs:
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω "Key: Value"
            text = div.get_text()
            matches = re.findall(r'([^:]+):\s*([^\n]+)', text)
            for key, value in matches:
                key = key.strip().lower().replace(' ', '_')
                value = value.strip()
                if key and value and len(key) < 50:
                    details[key] = value
        
        return details

    @staticmethod
    def extract_size(soup: BeautifulSoup) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–ª–æ—â–∞–¥—å –∏–∑ HTML (–µ—Å–ª–∏ –Ω–µ –±—ã–ª–æ –Ω–∞–π–¥–µ–Ω–æ –≤ API)"""
        # –ò—â–µ–º –ø–ª–æ—â–∞–¥—å –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        size_patterns = [
            re.compile(r'(\d{1,3}(?:,\d{3})*)\s*(?:sq\.?\s*ft\.?|SF|square\s*feet)', re.I),
            re.compile(r'(\d+\.?\d*)\s*acres?', re.I),
        ]
        
        page_text = soup.get_text()
        for pattern in size_patterns:
            match = pattern.search(page_text)
            if match:
                size_val = match.group(1)
                if 'acre' in pattern.pattern.lower():
                    return f"{size_val} acres"
                else:
                    return f"{size_val.replace(',', '')} SF"
        
        return None

    def _save_html_if_needed(self, html: str, listing_id: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç HTML –≤ —Ñ–∞–π–ª"""
        self.html_counter += 1
        if self.html_counter % self.save_html_every == 0:
            safe_filename = re.sub(r'[^\w\-_\.]', '_', listing_id)
            if not safe_filename or safe_filename == '_':
                safe_filename = f"listing_{self.html_counter}"
            filepath = os.path.join(self.html_save_dir, f"{safe_filename}.html")
            try:
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(html)
                logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω HTML [{self.html_counter}]: {filepath}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ HTML: {e}")

    async def run(self, location: str = "new-york", max_results: int = 1000) -> list[DbDTO]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ API (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)"""
        async with httpx.AsyncClient() as client:
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ API
            logger.info("[API MODE] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö")
            listings_data = await self.get_listings_from_api(client, location, max_results)
            
            if not listings_data:
                logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ API")
                return []
            
            # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ –∏–∑ API –æ—Ç–≤–µ—Ç–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            logger.info(f"[2-3] –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(listings_data)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ API...")
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            tasks = [
                self.parse_listing_from_api_data(client, listing_data)
                for listing_data in listings_data
            ]
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            results = []
            completed = 0
            for coro in asyncio.as_completed(tasks):
                try:
                    dto = await coro
                    completed += 1
                    if dto:
                        results.append(dto)
                        logger.info(f"‚úì [{completed}/{len(listings_data)}] –£—Å–ø–µ—à–Ω–æ: {dto.address}")
                    else:
                        logger.warning(f"‚ö† [{completed}/{len(listings_data)}] –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏–µ")
                except Exception as e:
                    completed += 1
                    logger.error(f"‚ùå [{completed}/{len(listings_data)}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
            
            logger.info(f"\n–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(results)}/{len(listings_data)}")
            return results

if __name__ == '__main__':
    async def main():
        parser = CompassParser()
        results = await parser.run("new-york", 5)
        print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(results)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
    
    asyncio.run(main())
