import asyncio
import json
import logging
import os
import re
import time
from typing import Any
from urllib.parse import urljoin, urlparse

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager

from schema import DbDTO, AgentData

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CompassParser:
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è compass.com —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Selenium
    1. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ –±—Ä–∞—É–∑–µ—Ä (–æ–±—Ö–æ–¥ –∑–∞—â–∏—Ç—ã)
    2. –ü–æ–ª—É—á–µ–Ω–∏–µ HTML –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ª–∏—Å—Ç–∏–Ω–≥–∞
    3. –ü–∞—Ä—Å–∏–Ω–≥ window.__INITIAL_DATA__ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
    """
    
    def __init__(
        self,
        headless: bool = True,
        save_html_every: int = 20,
        html_save_dir: str = "htmls",
        page_load_timeout: int = 30,
    ) -> None:
        self.source_name = "compass"
        self.base_url = "https://www.compass.com"
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Selenium
        self.chrome_options = Options()
        if headless:
            self.chrome_options.add_argument("--headless=new")
        
        self.chrome_options.add_argument("--no-sandbox")
        self.chrome_options.add_argument("--disable-dev-shm-usage")
        self.chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        self.chrome_options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        self.chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.chrome_options.add_experimental_option('useAutomationExtension', False)
        
        self.page_load_timeout = page_load_timeout
        self.driver = None
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML
        self.save_html_every = save_html_every
        self.html_save_dir = html_save_dir
        self.html_counter = 0
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        if not os.path.exists(self.html_save_dir):
            os.makedirs(self.html_save_dir)
            logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML: {self.html_save_dir}")

    def start_driver(self):
        """–ó–∞–ø—É—Å–∫ –¥—Ä–∞–π–≤–µ—Ä–∞"""
        if not self.driver:
            logger.info("–ó–∞–ø—É—Å–∫ Chrome driver...")
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=self.chrome_options)
            self.driver.set_page_load_timeout(self.page_load_timeout)
            
            # –ú–∞—Å–∫–∏—Ä–æ–≤–∫–∞ webdriver
            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    })
                '''
            })

    def stop_driver(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥—Ä–∞–π–≤–µ—Ä–∞"""
        if self.driver:
            logger.info("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ Chrome driver...")
            self.driver.quit()
            self.driver = None

    def get_page_source(self, url: str) -> str | None:
        """–ü–æ–ª—É—á–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ Selenium"""
        if not self.driver:
            self.start_driver()
        
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
            self.driver.get(url)
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏–ª–∏ —Å–ø–∏—Å–∫–∞)
            try:
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                # –î–∞–µ–º –≤—Ä–µ–º—è –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ JS —Å–∫—Ä–∏–ø—Ç–æ–≤
                time.sleep(2)
            except:
                pass
            
            return self.driver.page_source
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
            # –ï—Å–ª–∏ –¥—Ä–∞–π–≤–µ—Ä —É–ø–∞–ª, –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏–º –µ–≥–æ
            try:
                self.stop_driver()
            except:
                pass
            return None

    # ---------------------- –≠–¢–ê–ü 1: –ò–ù–î–ï–ö–°–ê–¶–ò–Ø ----------------------

    def get_listing_urls_from_search(self, location: str = "new-york", max_results: int = 1000) -> list[str]:
        """
        –≠–¢–ê–ü 1: –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–∏—Å–∫–∞ (Selenium)
        """
        logger.info(f"[1] –ü–æ–ª—É—á–∞—é —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–ª—è –ª–æ–∫–∞—Ü–∏–∏: {location}")
        
        if not self.driver:
            self.start_driver()
            
        urls = []
        page = 1
        
        try:
            while len(urls) < max_results:
                # URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞
                search_url = f"{self.base_url}/homes-for-sale/{location}/"
                if page > 1:
                    search_url += f"?page={page}"
                
                logger.info(f"[1] –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {search_url}")
                
                html = self.get_page_source(search_url)
                if not html:
                    logger.warning(f"[1] –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}")
                    break
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –ø–æ–∏—Å–∫–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                if page == 1:
                    debug_file = os.path.join(self.html_save_dir, f"search_page_{page}_selenium.html")
                    with open(debug_file, 'w', encoding='utf-8') as f:
                        f.write(html)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏
                new_urls = self._extract_urls_from_html(html)
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ
                page_urls = [url for url in new_urls if url not in urls]
                
                if not page_urls:
                    logger.info(f"[1] –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –ó–∞–≤–µ—Ä—à–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥.")
                    break
                
                urls.extend(page_urls)
                logger.info(f"[1] –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} –Ω–∞–π–¥–µ–Ω–æ {len(page_urls)} –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –í—Å–µ–≥–æ: {len(urls)}")
                
                # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                page += 1
                
                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
                if page > 50:
                    logger.warning(f"[1] –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü (50). –û—Å—Ç–∞–Ω–æ–≤–∫–∞.")
                    break
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
                time.sleep(2)
            
            logger.info(f"[1] –ò—Ç–æ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(urls)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
            return urls[:max_results]
            
        except Exception as e:
            logger.error(f"[1] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {e}")
            import traceback
            traceback.print_exc()
            return urls

    def _extract_urls_from_html(self, html: str) -> list[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ HTML"""
        urls = []
        soup = BeautifulSoup(html, 'lxml')
        
        # 1. –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —á–µ—Ä–µ–∑ __INITIAL_DATA__
        initial_data = self.extract_initial_data(html)
        if initial_data:
            listings = self.extract_listings_from_initial_data(initial_data)
            for listing in listings:
                url = None
                if isinstance(listing, dict):
                    # –ü—Ä–æ–±—É–µ–º —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å URL –∏–∑ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª–µ–π
                    listing_id = (
                        listing.get('id') or 
                        listing.get('listingId') or 
                        listing.get('mlsNumber') or
                        listing.get('listingIdSHA')
                    )
                    
                    # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å URL –∏–∑ location (–∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
                    if not url and listing.get('location') and listing.get('location', {}).get('seoId'):
                        # –§–æ—Ä–º–∞—Ç: /homes-for-sale/{seoId}/{listingIdSHA}/
                        seo_id = listing['location']['seoId']
                        sha_id = listing.get('listingIdSHA') or listing_id
                        if sha_id:
                            url = f"{self.base_url}/homes-for-sale/{seo_id}/{sha_id}/"
                    
                    # –ü—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π URL
                    if not url and listing_id:
                        url = f"{self.base_url}/homes-for-sale/{listing_id}/"
                        
                    # –ü—Ä–æ–±—É–µ–º –ø–æ–ª–µ pageLink (–∏–∑ –ø—Ä–∏–º–µ—Ä–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
                    if not url and listing.get('pageLink'):
                        url = listing['pageLink']
                
                if url:
                    if not url.startswith('http'):
                        url = urljoin(self.base_url, url)
                    if url not in urls:
                        urls.append(url)
        
        # 2. –ï—Å–ª–∏ –º–∞–ª–æ —Å—Å—ã–ª–æ–∫, –∏—â–µ–º –≤ HTML
        if len(urls) < 5:
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –≤–∏–¥–∞ /homes-for-sale/{location}/{id}/ –∏–ª–∏ /homes-for-sale/{id}/
            all_links = soup.find_all('a', href=True)
            known_locations = {
                'new-york', 'los-angeles', 'san-francisco', 'chicago', 
                'boston', 'miami', 'seattle', 'washington-dc', 'brooklyn',
                'manhattan', 'queens', 'bronx', 'staten-island'
            }
            
            for link in all_links:
                href = link.get('href', '')
                if not href or '=' in href or '?' in href or href.startswith('#'):
                    continue
                
                # –ü–∞—Ç—Ç–µ—Ä–Ω URL
                match = re.search(r'/homes-for-sale/([^/]+)/([^/]+)/?$', href)
                if match:
                    part1, part2 = match.groups()
                    listing_id = part2 if part1 in known_locations else part1
                    
                    if len(listing_id) > 5 and not listing_id.startswith(('start', 'page', 'sort')):
                        full_url = urljoin(self.base_url, href)
                        if full_url not in urls:
                            urls.append(full_url)
                            
        return urls

    @staticmethod
    def extract_initial_data(html: str) -> dict[str, Any] | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç window.__INITIAL_DATA__ –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            soup = BeautifulSoup(html, 'lxml')
            scripts = soup.find_all('script')
            
            for script in scripts:
                script_text = script.string or script.get_text()
                if not script_text:
                    continue
                
                # –ò—â–µ–º window.__INITIAL_DATA__ = {...}
                # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω, —É—á–∏—Ç—ã–≤–∞—é—â–∏–π –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
                patterns = [
                    r'window\.__INITIAL_DATA__\s*=\s*({.+?});',
                    r'__INITIAL_DATA__\s*=\s*({.+?});',
                ]
                
                for pattern in patterns:
                    matches = list(re.finditer(pattern, script_text, re.DOTALL))
                    for match in matches:
                        json_str = match.group(1).strip()
                        try:
                            # –ü—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
                            data = json.loads(json_str)
                            if isinstance(data, dict) and len(data) > 0:
                                return data
                        except json.JSONDecodeError:
                            pass
            return None
        except Exception:
            return None

    @staticmethod
    def extract_listings_from_initial_data(data: dict[str, Any]) -> list[dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ __INITIAL_DATA__"""
        listings = []
        
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∫–ª—é—á–∞ 'listings' –∏–ª–∏ 'listing'
        def find_key(obj, key):
            if isinstance(obj, dict):
                if key in obj:
                    return obj[key]
                for k, v in obj.items():
                    res = find_key(v, key)
                    if res: return res
            elif isinstance(obj, list):
                for item in obj:
                    res = find_key(item, key)
                    if res: return res
            return None

        # –ò—â–µ–º —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        found = find_key(data, 'listings') or find_key(data, 'cards')
        if found and isinstance(found, list):
            listings = found
            
        return listings

    # ---------------------- –≠–¢–ê–ü 3: –ü–ê–†–°–ò–ù–ì –î–ê–ù–ù–´–• ----------------------

    def parse_listing(self, url: str) -> DbDTO | None:
        """
        –≠–¢–ê–ü 3: –ü–∞—Ä—Å–∏—Ç HTML –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –∏–∑ window.__INITIAL_DATA__
        """
        html = self.get_page_source(url)
        if not html:
            return None
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ URL –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞
        listing_id = "unknown"
        parsed = urlparse(url)
        path_parts = [p for p in parsed.path.split('/') if p]
        if path_parts:
            listing_id = path_parts[-1]
        
        self._save_html_if_needed(html, listing_id)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        initial_data = self.extract_initial_data(html)
        if not initial_data:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å __INITIAL_DATA__ –¥–ª—è {url}")
            return None
        
        # –ò—â–µ–º –æ–±—ä–µ–∫—Ç –ª–∏—Å—Ç–∏–Ω–≥–∞ –≤–Ω—É—Ç—Ä–∏ –¥–∞–Ω–Ω—ã—Ö
        listing_data = self._find_listing_data(initial_data)
        if not listing_data:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –≤ JSON –¥–ª—è {url}")
            return None
            
        # –ú–∞–ø–ø–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –≤ DbDTO
        try:
            return self._map_to_dto(listing_data, url, listing_id)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –º–∞–ø–ø–∏–Ω–≥–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {url}: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _find_listing_data(self, data: dict) -> dict | None:
        """–ù–∞—Ö–æ–¥–∏—Ç –¥–∞–Ω–Ω—ã–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ"""
        # –û–±—ã—á–Ω–æ —ç—Ç–æ listingRelation -> listing –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ listing
        if 'listingRelation' in data and 'listing' in data['listingRelation']:
            return data['listingRelation']['listing']
        
        if 'listing' in data:
            return data['listing']
            
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
        def find_listing(obj):
            if isinstance(obj, dict):
                if 'listingIdSHA' in obj or 'compassPropertyId' in obj:
                    return obj
                for k, v in obj.items():
                    res = find_listing(v)
                    if res: return res
            return None
            
        return find_listing(data)

    def _map_to_dto(self, data: dict, url: str, listing_id: str) -> DbDTO:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç JSON –¥–∞–Ω–Ω—ã–µ –≤ DbDTO"""
        
        # Helper –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π
        def get_val(obj, path, default=None):
            for key in path.split('.'):
                if isinstance(obj, dict) and key in obj:
                    obj = obj[key]
                else:
                    return default
            return obj

        # Address
        location = data.get('location', {})
        address = location.get('prettyAddress') or get_val(data, 'address.prettyAddress') or "Address not found"
        
        # Price
        price_val = get_val(data, 'price.listed') or get_val(data, 'price.lastKnown')
        price_str = f"${price_val:,.0f}" if price_val else None
        
        # Type
        listing_type_code = data.get('listingType')
        listing_type = 'sale'  # Default
        if listing_type_code == 1: listing_type = 'lease' # –ü—Ä–∏–º–µ—Ä, –Ω—É–∂–Ω–æ —É—Ç–æ—á–Ω—è—Ç—å –∫–æ–¥—ã
        if 'rent' in str(data.get('detailedPropertyType', '')).lower():
            listing_type = 'lease'
            
        sale_price = price_str if listing_type == 'sale' else None
        lease_price = price_str if listing_type == 'lease' else None
        
        # Size
        size_sqft = get_val(data, 'size.squareFeet')
        size = f"{size_sqft:,.0f} SF" if size_sqft else None
        
        # Description
        description = data.get('description')
        
        # Photos
        photos = []
        media = data.get('media', [])
        for item in media:
            if item.get('type') == 0 and item.get('url'): # 0 = image
                photos.append(item['url'])
        
        # –ï—Å–ª–∏ —Ñ–æ—Ç–æ –Ω–µ –≤ media, –∏—â–µ–º –≤ –¥—Ä—É–≥–∏—Ö –º–µ—Å—Ç–∞—Ö
        if not photos:
            # Compass —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç gallery
            gallery = get_val(data, 'gallery', [])
            for item in gallery:
                if item.get('url'): photos.append(item['url'])

        # Agents
        agents = []
        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –∏–∑ listing.agents
        
        # Status
        status = data.get('localizedStatus') or "Available"
        
        # Details
        details = {
            'bedrooms': get_val(data, 'size.bedrooms'),
            'bathrooms': get_val(data, 'size.bathrooms'),
            'year_built': get_val(data, 'yearBuilt'),
            'property_type': get_val(data, 'propertyType.name'),
        }

        return DbDTO(
            source_name=self.source_name,
            listing_id=listing_id,
            listing_link=url,
            listing_type=listing_type,
            listing_status=status,
            address=address,
            sale_price=sale_price,
            lease_price=lease_price,
            size=size,
            property_description=description,
            listing_details=details,
            photos=photos,
            brochure_pdf=None,
            mls_number=None,
            agents=agents,
            agency_phone=None,
        )

    def _save_html_if_needed(self, html: str, listing_id: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç HTML –≤ —Ñ–∞–π–ª"""
        self.html_counter += 1
        if self.html_counter % self.save_html_every == 0:
            safe_filename = re.sub(r'[^\w\-_\.]', '_', listing_id)
            if not safe_filename or safe_filename == '_':
                safe_filename = f"listing_{self.html_counter}"
            filepath = os.path.join(self.html_save_dir, f"{safe_filename}.html")
            try:
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(html)
                logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω HTML [{self.html_counter}]: {filepath}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ HTML: {e}")

    def run(self, location: str = "new-york", max_results: int = 1000) -> list[DbDTO]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å"""
        try:
            # 1. –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏
            urls = self.get_listing_urls_from_search(location, max_results)
            if not urls:
                logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
                return []
            
            # 2. –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
            results = []
            logger.info(f"[2-3] –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(urls)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π...")
            
            for i, url in enumerate(urls):
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ [{i+1}/{len(urls)}]: {url}")
                dto = self.parse_listing(url)
                if dto:
                    results.append(dto)
                    logger.info(f"‚úì –£—Å–ø–µ—à–Ω–æ: {dto.address}")
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(1)
                
            logger.info(f"\n–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(results)}/{len(urls)}")
            return results
            
        finally:
            self.stop_driver()

if __name__ == '__main__':
    parser = CompassParser(headless=False) # Headless=False –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    parser.run("new-york", 5)
