import asyncio
import json
import logging
import os
import re
import xml.etree.ElementTree as ET
from typing import Any
from urllib.parse import urljoin, urlparse

import httpx
from bs4 import BeautifulSoup
from fake_useragent import UserAgent

from schema import DbDTO, AgentData

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class RwholmesParser:
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è rwholmes.com
    1. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è - —Å–±–æ—Ä –≤—Å–µ—Ö URL /property/{listing_id}
    2. –ü–æ–ª—É—á–µ–Ω–∏–µ HTML –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ª–∏—Å—Ç–∏–Ω–≥–∞
    3. –ü–∞—Ä—Å–∏–Ω–≥ HTML –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
    """
    
    def __init__(
        self,
        client: httpx.AsyncClient,
        source_name: str = "rwholmes",
        concurrency: int = 10,
        save_html_every: int = 20,
        html_save_dir: str = "htmls",
    ) -> None:
        self.client = client
        self.source_name = source_name
        self.semaphore = asyncio.Semaphore(concurrency)
        
        self.sitemap_url = "https://rwholmes.com/estate_property-sitemap.xml"
        self.base_url = "https://rwholmes.com"
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML
        self.save_html_every = save_html_every
        self.html_save_dir = html_save_dir
        self.html_counter = 0
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        if not os.path.exists(self.html_save_dir):
            os.makedirs(self.html_save_dir)
            logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML: {self.html_save_dir}")

    # ---------------------- NETWORK ----------------------

    def get_headers(self) -> dict[str, str]:
        return {
            "User-Agent": UserAgent().random,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Referer": "https://rwholmes.com/estate_property-sitemap.xml",
            "Sec-GPC": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "same-origin",
            "Sec-Fetch-User": "?1",
        }

    async def get_html(self, url: str) -> str | None:
        """–ü–æ–ª—É—á–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            async with self.semaphore:
                resp = await self.client.get(
                    url,
                    headers=self.get_headers(),
                    timeout=10.0,
                    follow_redirects=True,  # –°–ª–µ–¥–æ–≤–∞—Ç—å —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º
                )
                resp.raise_for_status()
                return resp.text
        except Exception as e:
            logger.error(f"GET {url} failed: {e}")
            return None

    # ---------------------- –≠–¢–ê–ü 1: –ò–ù–î–ï–ö–°–ê–¶–ò–Ø ----------------------

    async def get_listing_urls(self) -> list[str]:
        """
        –≠–¢–ê–ü 1: –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ URL –≤–∏–¥–∞ /property/{listing_id} –∏–∑ sitemap
        """
        logger.info(f"[1] –ü–æ–ª—É—á–∞—é sitemap: {self.sitemap_url}")
        
        try:
            resp = await self.client.get(
                self.sitemap_url, 
                headers=self.get_headers(),
                follow_redirects=True
            )
            resp.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º XML
            root = ET.fromstring(resp.content)
            namespace = {'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
            urls = []
            
            for url_elem in root.findall('.//sitemap:url', namespace):
                loc = url_elem.find('sitemap:loc', namespace)
                if loc is not None:
                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç, —É—á–∏—Ç—ã–≤–∞—è CDATA
                    url = loc.text
                    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç None, –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —á–µ—Ä–µ–∑ tostring
                    if url is None:
                        url_text = ET.tostring(loc, encoding='unicode', method='text')
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –º–µ–∂–¥—É CDATA –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç
                        url = url_text.strip() if url_text else None
                    
                    if url and '/propert' in url.lower():  # –ò—â–µ–º /property –∏–ª–∏ /properties
                        urls.append(url.strip())
            
            logger.info(f"[1] –ù–∞–π–¥–µ–Ω–æ {len(urls)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
            return urls
        except Exception as e:
            logger.error(f"[1] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ sitemap: {e}")
            return []

    @staticmethod
    def extract_listing_id_from_url(url: str) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç listing_id –∏–∑ URL"""
        parsed = urlparse(url)
        path_parts = [p for p in parsed.path.split('/') if p]
        if path_parts:
            # –ò—â–µ–º —á–∞—Å—Ç—å –ø–æ—Å–ª–µ /property/ –∏–ª–∏ /properties/
            for i, part in enumerate(path_parts):
                if part.lower() in ['property', 'properties'] and i + 1 < len(path_parts):
                    return path_parts[i + 1]
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∞—Å—Ç—å
            return path_parts[-1]
        return None

    # ---------------------- –≠–¢–ê–ü 2: –ü–û–õ–£–ß–ï–ù–ò–ï HTML ----------------------

    async def get_listing_html(self, url: str) -> str | None:
        """
        –≠–¢–ê–ü 2: –ü–æ–ª—É—á–∞–µ—Ç HTML –¥–ª—è –ª–∏—Å—Ç–∏–Ω–≥–∞
        """
        return await self.get_html(url)

    # ---------------------- –≠–¢–ê–ü 3: –ü–ê–†–°–ò–ù–ì –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–• –ü–û–õ–ï–ô ----------------------

    @staticmethod
    def extract_mls(soup: BeautifulSoup) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç MLS –Ω–æ–º–µ—Ä"""
        mls = None
        
        mls_patterns = [
            re.compile(r'MLS[#:\s]*([A-Z0-9\-]+)', re.I),
            re.compile(r'MLS\s*Number[#:\s]*([A-Z0-9\-]+)', re.I),
            re.compile(r'Multiple\s*Listing\s*Service[#:\s]*([A-Z0-9\-]+)', re.I),
        ]
        
        page_text = soup.get_text()
        for pattern in mls_patterns:
            match = pattern.search(page_text)
            if match:
                mls = match.group(1).strip()
                break
        
        if not mls:
            mls_elements = soup.find_all(string=re.compile(r'MLS', re.I))
            for elem in mls_elements:
                parent = elem.parent if hasattr(elem, 'parent') else None
                if parent:
                    text = parent.get_text()
                    for pattern in mls_patterns:
                        match = pattern.search(text)
                        if match:
                            mls = match.group(1).strip()
                            break
                    if mls:
                        break
        
        return mls

    @staticmethod
    def extract_details(soup: BeautifulSoup) -> dict[str, str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É listing_details –∫–∞–∫ —Å–ª–æ–≤–∞—Ä—å"""
        details = {}
        
        # 1. HTML —Ç–∞–±–ª–∏—Ü—ã
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    key = cells[0].get_text(strip=True).lower()
                    value = cells[1].get_text(strip=True)
                    if key and value:
                        details[key] = value
        
        # 2. –°–ø–∏—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π (dl)
        dl_lists = soup.find_all('dl')
        for dl in dl_lists:
            dts = dl.find_all('dt')
            dds = dl.find_all('dd')
            for dt, dd in zip(dts, dds):
                key = dt.get_text(strip=True).lower()
                value = dd.get_text(strip=True)
                if key and value:
                    details[key] = value
        
        # 3. Div'—ã —Å –ø–∞—Ä–∞–º–∏ –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ
        detail_divs = soup.find_all(class_=re.compile(r'detail|spec|feature|property-info', re.I))
        for div in detail_divs:
            text = div.get_text()
            matches = re.findall(r'([^:]+):\s*([^\n]+)', text)
            for key, value in matches:
                key = key.strip().lower()
                value = value.strip()
                if key and value:
                    details[key] = value
        
        return details

    @staticmethod
    def extract_price(soup: BeautifulSoup) -> tuple[str | None, str | None]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–Ω—É –∏ —Ç–∏–ø (sale/lease)
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (price_value, listing_type)
        """
        price_value = None
        listing_type = None
        
        price_pattern = re.compile(r'\$[\d,]+(?:\.[\d]+)?')
        price_elements = soup.find_all(string=price_pattern)
        
        for price_text in price_elements:
            price_str = str(price_text).strip()
            match = price_pattern.search(price_str)
            if match:
                parent = price_text.parent if hasattr(price_text, 'parent') else None
                context = parent.get_text().lower() if parent else price_str.lower()
                
                if any(word in context for word in ['sale', 'for sale', 'asking', 'purchase', 'buy']):
                    price_value = match.group(0)
                    listing_type = 'sale'
                    break
                elif any(word in context for word in ['lease', 'rent', 'rental', 'monthly', 'annual', 'per sqft']):
                    price_value = match.group(0)
                    listing_type = 'lease'
                    break
        
        if not price_value:
            price_divs = soup.find_all(class_=re.compile(r'price|cost|amount', re.I))
            for price_div in price_divs:
                price_text = price_div.get_text(strip=True)
                match = price_pattern.search(price_text)
                if match:
                    price_value = match.group(0)
                    class_name = ' '.join(price_div.get('class', [])).lower()
                    if 'sale' in class_name or 'purchase' in class_name:
                        listing_type = 'sale'
                    elif 'lease' in class_name or 'rent' in class_name:
                        listing_type = 'lease'
                    break
        
        return price_value, listing_type

    @staticmethod
    def extract_size(soup: BeautifulSoup) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–ª–æ—â–∞–¥—å –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Ñ—É—Ç–∞—Ö"""
        size = None
        
        size_patterns = [
            re.compile(r'([\d,]+(?:\.[\d]+)?)[\s]*sq\.?ft\.?', re.I),
            re.compile(r'([\d,]+(?:\.[\d]+)?)[\s]*square[\s]*feet', re.I),
            re.compile(r'([\d,]+(?:\.[\d]+)?)[\s]*sf\b', re.I),
            re.compile(r'size[:\s]+([\d,]+(?:\.[\d]+)?)', re.I),
        ]
        
        page_text = soup.get_text()
        for pattern in size_patterns:
            match = pattern.search(page_text)
            if match:
                size = match.group(0).strip()
                break
        
        return size

    @staticmethod
    def extract_address(soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–¥—Ä–µ—Å –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)"""
        address = None
        
        # 1. H1 –∑–∞–≥–æ–ª–æ–≤–æ–∫ (—á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –∞–¥—Ä–µ—Å)
        h1 = soup.find('h1')
        if h1:
            h1_text = h1.get_text(strip=True)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ –∞–¥—Ä–µ—Å (—Å–æ–¥–µ—Ä–∂–∏—Ç —Ü–∏—Ñ—Ä—ã –∏ —É–ª–∏—Ü—É)
            if re.search(r'\d+', h1_text) and len(h1_text) > 10:
                address = h1_text
        
        # 2. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã –¥–ª—è –∞–¥—Ä–µ—Å–∞
        if not address:
            address_selectors = [
                soup.find(class_=re.compile(r'address|property-address|location|street', re.I)),
                soup.find(id=re.compile(r'address|property-address|location', re.I)),
                soup.find('div', {'itemprop': 'address'}),
                soup.find('span', {'itemprop': 'address'}),
            ]
            
            for selector in address_selectors:
                if selector:
                    addr_text = selector.get_text(strip=True)
                    if len(addr_text) > 5 and re.search(r'\d+', addr_text):
                        address = addr_text
                        break
        
        # 3. Title —Ç–µ–≥ (–º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∞–¥—Ä–µ—Å)
        if not address:
            title_tag = soup.find('title')
            if title_tag:
                title_text = title_tag.get_text(strip=True)
                # –ò—â–µ–º –∞–¥—Ä–µ—Å –≤ title (–æ–±—ã—á–Ω–æ –≤ –Ω–∞—á–∞–ª–µ –∏–ª–∏ –∫–æ–Ω—Ü–µ)
                if re.search(r'\d+.*(street|st|avenue|ave|road|rd|drive|dr|boulevard|blvd|way|ln)', title_text, re.I):
                    address = title_text.split('|')[0].strip()
        
        # 4. –ú–µ—Ç–∞-—Ç–µ–≥–∏
        if not address:
            meta_address = soup.find('meta', {'property': 'og:street-address'})
            if meta_address and meta_address.get('content'):
                address = meta_address['content']
        
        # 5. –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –∏–ª–∏ –ø–µ—Ä–≤–æ–≥–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
        if not address:
            desc = soup.find(class_=re.compile(r'description|summary|details', re.I))
            if desc:
                desc_text = desc.get_text()
                # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –∞–¥—Ä–µ—Å–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
                addr_match = re.search(r'(\d+\s+[\w\s]+(?:street|st|avenue|ave|road|rd|drive|dr|boulevard|blvd|way|ln)[,\s]+[\w\s]+[,\s]+[A-Z]{2}\s+\d{5})', desc_text, re.I)
                if addr_match:
                    address = addr_match.group(1).strip()
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∞–¥—Ä–µ—Å –∏–ª–∏ fallback –Ω–∞ URL –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏
        return address or "Address not found"

    @staticmethod
    def extract_description(soup: BeautifulSoup) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
        description = None
        
        # 1. –ú–µ—Ç–∞-—Ç–µ–≥ description
        meta_desc = soup.find('meta', {'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            description = meta_desc['content'].strip()
        
        # 2. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –±–ª–æ–∫–∏
        if not description:
            desc_selectors = [
                soup.find(class_=re.compile(r'description|about|details|summary', re.I)),
                soup.find(id=re.compile(r'description|about|details', re.I)),
                soup.find('div', {'itemprop': 'description'}),
            ]
            
            for desc_elem in desc_selectors:
                if desc_elem:
                    description = desc_elem.get_text(strip=True)
                    if len(description) > 50:
                        break
        
        # 3. –ü–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        if not description:
            paragraphs = soup.find_all('p')
            for p in paragraphs:
                text = p.get_text(strip=True)
                if len(text) > 100:
                    description = text
                    break
        
        return description

    @staticmethod
    def extract_listing_status(soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        status = None
        
        status_keywords = {
            'coming soon': ['coming soon', 'coming-soon', 'soon'],
            'available': ['available', 'active', 'for sale', 'for lease', 'on market'],
            'sold': ['sold', 'sale pending'],
            'leased': ['leased', 'rented'],
            'off market': ['off market', 'off-market', 'withdrawn'],
            'pending': ['pending', 'under contract'],
        }
        
        page_text = soup.get_text().lower()
        
        status_elements = soup.find_all(class_=re.compile(r'status|availability|listing-status', re.I))
        for elem in status_elements:
            text = elem.get_text().lower()
            for status_name, keywords in status_keywords.items():
                if any(keyword in text for keyword in keywords):
                    status = status_name
                    break
            if status:
                break
        
        if not status:
            for status_name, keywords in status_keywords.items():
                if any(keyword in page_text for keyword in keywords):
                    status = status_name
                    break
        
        return status or 'available'

    @staticmethod
    def extract_photos(soup: BeautifulSoup, base_url: str) -> list[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–æ—Ç–æ"""
        photos = []
        
        # 1. –ì–∞–ª–µ—Ä–µ—è
        gallery = soup.find(class_=re.compile(r'gallery|slider|carousel|images|photos', re.I))
        if gallery:
            gallery_imgs = gallery.find_all('img')
            for img in gallery_imgs:
                src = img.get('src') or img.get('data-src') or img.get('data-lazy-src') or img.get('data-original')
                if src:
                    full_url = urljoin(base_url, src)
                    if full_url not in photos and 'placeholder' not in full_url.lower():
                        photos.append(full_url)
        
        # 2. –í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if not photos:
            images = soup.find_all('img')
            for img in images:
                src = img.get('src') or img.get('data-src') or img.get('data-lazy-src') or img.get('data-original')
                if src:
                    full_url = urljoin(base_url, src)
                    if (full_url not in photos and 
                        'logo' not in full_url.lower() and 
                        'icon' not in full_url.lower() and
                        'placeholder' not in full_url.lower() and
                        'avatar' not in full_url.lower()):
                        photos.append(full_url)
        
        return photos

    @staticmethod
    def extract_brochure_pdf(soup: BeautifulSoup, base_url: str) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ brochure PDF"""
        brochure_url = None
        
        pdf_links = soup.find_all('a', href=re.compile(r'\.pdf', re.I))
        for link in pdf_links:
            href = link.get('href', '')
            text = link.get_text().lower()
            
            if any(keyword in text for keyword in ['brochure', 'flyer', 'marketing', 'property']):
                brochure_url = urljoin(base_url, href)
                break
        
        if not brochure_url and pdf_links:
            href = pdf_links[0].get('href', '')
            brochure_url = urljoin(base_url, href)
        
        return brochure_url

    def _save_html_if_needed(self, html: str, listing_id: str, url: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç HTML –≤ —Ñ–∞–π–ª, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ (–∫–∞–∂–¥—ã–π N-–π)"""
        self.html_counter += 1
        
        if self.html_counter % self.save_html_every == 0:
            # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ listing_id
            safe_filename = re.sub(r'[^\w\-_\.]', '_', listing_id)
            if not safe_filename or safe_filename == '_':
                safe_filename = f"listing_{self.html_counter}"
            
            filepath = os.path.join(self.html_save_dir, f"{safe_filename}.html")
            
            try:
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(html)
                logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω HTML [{self.html_counter}]: {filepath}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ HTML {filepath}: {e}")

    async def parse_listing(self, url: str) -> DbDTO | None:
        """
        –≠–¢–ê–ü 3: –ü–∞—Ä—Å–∏—Ç HTML –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DbDTO –æ–±—ä–µ–∫—Ç
        """
        # –ü–æ–ª—É—á–∞–µ–º HTML
        html = await self.get_listing_html(url)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'lxml')
        listing_id = self.extract_listing_id_from_url(url) or url
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–π N-–π HTML
        self._save_html_if_needed(html, listing_id, url)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        price, listing_type = self.extract_price(soup)
        size = self.extract_size(soup)
        description = self.extract_description(soup)
        listing_status = self.extract_listing_status(soup)
        listing_details = self.extract_details(soup)
        photos = self.extract_photos(soup, self.base_url)
        brochure_pdf = self.extract_brochure_pdf(soup, self.base_url)
        mls_number = self.extract_mls(soup)
        address = self.extract_address(soup)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º —Ü–µ–Ω—É –Ω–∞ sale_price –∏ lease_price –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
        sale_price = None
        lease_price = None
        if price:
            if listing_type == 'sale':
                sale_price = price
            elif listing_type == 'lease':
                lease_price = price
            else:
                # –ï—Å–ª–∏ —Ç–∏–ø –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω, –ø—ã—Ç–∞–µ–º—Å—è —É–≥–∞–¥–∞—Ç—å –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
                # –ò–ª–∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –æ–±–∞ –ø–æ–ª—è
                sale_price = price
                lease_price = price
        
        # –°–æ–∑–¥–∞–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º DbDTO –æ–±—ä–µ–∫—Ç
        try:
            dto = DbDTO(
                source_name=self.source_name,
                listing_id=listing_id,
                listing_link=url,
                listing_type=listing_type,
                listing_status=listing_status,
                address=address,  # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ
                sale_price=sale_price,
                lease_price=lease_price,
                size=size,
                property_description=description,
                listing_details=listing_details if listing_details else None,
                photos=photos if photos else None,
                brochure_pdf=brochure_pdf,
                mls_number=mls_number,
            )
            return dto
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ DbDTO –¥–ª—è {url}: {e}")
            return None

    # ---------------------- –û–°–ù–û–í–ù–û–ô –ü–†–û–¶–ï–°–° ----------------------

    async def run(self) -> list[DbDTO]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å:
        1. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è - —Å–±–æ—Ä –≤—Å–µ—Ö URL
        2. –ü–æ–ª—É—á–µ–Ω–∏–µ HTML –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ª–∏—Å—Ç–∏–Ω–≥–∞
        3. –ü–∞—Ä—Å–∏–Ω–≥ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ DbDTO –æ–±—ä–µ–∫—Ç–æ–≤
        """
        # –≠–¢–ê–ü 1: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è
        listing_urls = await self.get_listing_urls()
        
        if not listing_urls:
            logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
            return []
        
        # –≠–¢–ê–ü 2 –∏ 3: –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ –ª–∏—Å—Ç–∏–Ω–≥–∞
        results: list[DbDTO] = []
        tasks = [self.parse_listing(url) for url in listing_urls]
        
        logger.info(f"[2-3] –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(listing_urls)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π...")
        
        parsed_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for i, result in enumerate(parsed_results):
            if isinstance(result, Exception):
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {listing_urls[i]}: {result}")
                continue
            
            if result and isinstance(result, DbDTO):
                results.append(result)
                logger.info(f"‚úì [{len(results)}/{len(listing_urls)}] {result.listing_id}")
        
        logger.info(f"\n–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(results)}/{len(listing_urls)}")
        return results


# ---------------------- –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø ----------------------

async def main():
    async with httpx.AsyncClient() as client:
        parser = RwholmesParser(client, concurrency=10)
        results = await parser.run()
        
        if results:
            print("\n–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–≤–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è:")
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º DbDTO –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –≤—ã–≤–æ–¥–∞
            first_dto = results[0]
            print(json.dumps(first_dto.model_dump(), indent=2, ensure_ascii=False))
        
        return results


if __name__ == '__main__':
    asyncio.run(main())
