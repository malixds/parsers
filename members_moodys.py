import asyncio
import json
import logging
import os
import re
import xml.etree.ElementTree as ET
from typing import Any
from urllib.parse import urljoin, urlparse

import httpx
from bs4 import BeautifulSoup
from fake_useragent import UserAgent

from schema import DbDTO, AgentData

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class RwholmesParser:
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è rwholmes.com
    1. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è - —Å–±–æ—Ä –≤—Å–µ—Ö URL /property/{listing_id}
    2. –ü–æ–ª—É—á–µ–Ω–∏–µ HTML –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ª–∏—Å—Ç–∏–Ω–≥–∞
    3. –ü–∞—Ä—Å–∏–Ω–≥ HTML –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
    """
    
    def __init__(
        self,
        client: httpx.AsyncClient,
        source_name: str = "rwholmes",
        concurrency: int = 10,
        save_html_every: int = 20,
        html_save_dir: str = "htmls",
    ) -> None:
        self.client = client
        self.source_name = source_name
        self.semaphore = asyncio.Semaphore(concurrency)
        
        self.sitemap_url = "https://rwholmes.com/estate_property-sitemap.xml"
        self.base_url = "https://rwholmes.com"
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML
        self.save_html_every = save_html_every
        self.html_save_dir = html_save_dir
        self.html_counter = 0
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        if not os.path.exists(self.html_save_dir):
            os.makedirs(self.html_save_dir)
            logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML: {self.html_save_dir}")

    # ---------------------- NETWORK ----------------------

    def get_headers(self) -> dict[str, str]:
        return {
            "User-Agent": UserAgent().random,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Referer": "https://rwholmes.com/estate_property-sitemap.xml",
            "Sec-GPC": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "same-origin",
            "Sec-Fetch-User": "?1",
        }

    async def get_html(self, url: str) -> str | None:
        """–ü–æ–ª—É—á–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            async with self.semaphore:
                resp = await self.client.get(
                    url,
                    headers=self.get_headers(),
                    timeout=10.0,
                    follow_redirects=True,  # –°–ª–µ–¥–æ–≤–∞—Ç—å —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º
                )
                resp.raise_for_status()
                return resp.text
        except Exception as e:
            logger.error(f"GET {url} failed: {e}")
            return None

    # ---------------------- –≠–¢–ê–ü 1: –ò–ù–î–ï–ö–°–ê–¶–ò–Ø ----------------------

    async def get_listing_urls(self) -> list[str]:
        """
        –≠–¢–ê–ü 1: –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ URL –≤–∏–¥–∞ /property/{listing_id} –∏–∑ sitemap
        """
        logger.info(f"[1] –ü–æ–ª—É—á–∞—é sitemap: {self.sitemap_url}")
        
        try:
            resp = await self.client.get(
                self.sitemap_url, 
                headers=self.get_headers(),
                follow_redirects=True
            )
            resp.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º XML
            root = ET.fromstring(resp.content)
            namespace = {'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
            urls = []
            
            for url_elem in root.findall('.//sitemap:url', namespace):
                loc = url_elem.find('sitemap:loc', namespace)
                if loc is not None:
                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç, —É—á–∏—Ç—ã–≤–∞—è CDATA
                    url = loc.text
                    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç None, –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —á–µ—Ä–µ–∑ tostring
                    if url is None:
                        url_text = ET.tostring(loc, encoding='unicode', method='text')
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –º–µ–∂–¥—É CDATA –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç
                        url = url_text.strip() if url_text else None
                    
                    if url and '/propert' in url.lower():  # –ò—â–µ–º /property –∏–ª–∏ /properties
                        urls.append(url.strip())
            
            logger.info(f"[1] –ù–∞–π–¥–µ–Ω–æ {len(urls)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
            return urls
        except Exception as e:
            logger.error(f"[1] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ sitemap: {e}")
            return []

    @staticmethod
    def extract_listing_id_from_url(url: str) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç listing_id –∏–∑ URL"""
        parsed = urlparse(url)
        path_parts = [p for p in parsed.path.split('/') if p]
        if path_parts:
            # –ò—â–µ–º —á–∞—Å—Ç—å –ø–æ—Å–ª–µ /property/ –∏–ª–∏ /properties/
            for i, part in enumerate(path_parts):
                if part.lower() in ['property', 'properties'] and i + 1 < len(path_parts):
                    return path_parts[i + 1]
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∞—Å—Ç—å
            return path_parts[-1]
        return None

    # ---------------------- –≠–¢–ê–ü 2: –ü–û–õ–£–ß–ï–ù–ò–ï HTML ----------------------

    async def get_listing_html(self, url: str) -> str | None:
        """
        –≠–¢–ê–ü 2: –ü–æ–ª—É—á–∞–µ—Ç HTML –¥–ª—è –ª–∏—Å—Ç–∏–Ω–≥–∞
        """
        return await self.get_html(url)

    # ---------------------- –≠–¢–ê–ü 3: –ü–ê–†–°–ò–ù–ì –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–• –ü–û–õ–ï–ô ----------------------

    @staticmethod
    def extract_mls(soup: BeautifulSoup) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç MLS –Ω–æ–º–µ—Ä"""
        mls = None
        
        mls_patterns = [
            re.compile(r'MLS[#:\s]*([A-Z0-9\-]+)', re.I),
            re.compile(r'MLS\s*Number[#:\s]*([A-Z0-9\-]+)', re.I),
            re.compile(r'Multiple\s*Listing\s*Service[#:\s]*([A-Z0-9\-]+)', re.I),
        ]
        
        page_text = soup.get_text()
        for pattern in mls_patterns:
            match = pattern.search(page_text)
            if match:
                mls = match.group(1).strip()
                break
        
        if not mls:
            mls_elements = soup.find_all(string=re.compile(r'MLS', re.I))
            for elem in mls_elements:
                parent = elem.parent if hasattr(elem, 'parent') else None
                if parent:
                    text = parent.get_text()
                    for pattern in mls_patterns:
                        match = pattern.search(text)
                        if match:
                            mls = match.group(1).strip()
                            break
                    if mls:
                        break
        
        return mls

    @staticmethod
    def extract_details(soup: BeautifulSoup) -> dict[str, str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É listing_details –∫–∞–∫ —Å–ª–æ–≤–∞—Ä—å"""
        details = {}
        
        # 1. HTML —Ç–∞–±–ª–∏—Ü—ã
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    key = cells[0].get_text(strip=True).lower()
                    value = cells[1].get_text(strip=True)
                    if key and value:
                        details[key] = value
        
        # 2. –°–ø–∏—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π (dl)
        dl_lists = soup.find_all('dl')
        for dl in dl_lists:
            dts = dl.find_all('dt')
            dds = dl.find_all('dd')
            for dt, dd in zip(dts, dds):
                key = dt.get_text(strip=True).lower()
                value = dd.get_text(strip=True)
                if key and value:
                    details[key] = value
        
        # 3. Div'—ã —Å –ø–∞—Ä–∞–º–∏ –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ
        detail_divs = soup.find_all(class_=re.compile(r'detail|spec|feature|property-info', re.I))
        for div in detail_divs:
            text = div.get_text()
            matches = re.findall(r'([^:]+):\s*([^\n]+)', text)
            for key, value in matches:
                key = key.strip().lower()
                value = value.strip()
                if key and value:
                    details[key] = value

        # 4. –§–æ–ª–ª–±–µ–∫: –ø–∞—Ä—Å–∏–º —ç–ª–µ–º–µ–Ω—Ç—ã <li> —Å —à–∞–±–ª–æ–Ω–æ–º "–∫–ª—é—á: –∑–Ω–∞—á–µ–Ω–∏–µ"
        # –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç, –µ—Å–ª–∏ –¥–µ—Ç–∞–ª–∏ –∑–∞–¥–∞–Ω—ã –≤ —Å–ø–∏—Å–∫–∞—Ö, –∞ –Ω–µ –≤ —Ç–∞–±–ª–∏—Ü–µ.
        for li in soup.find_all("li"):
            text = li.get_text(" ", strip=True)
            if ":" not in text:
                continue
            key, value = text.split(":", 1)
            key = key.strip().lower()
            value = value.strip()
            # –ù–µ–º–Ω–æ–≥–æ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —à—É–º: –∫–ª—é—á –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π –∏ –µ—Å—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ
            if key and value and len(key) <= 60:
                # –ù–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª—é—á–∏
                if key not in details:
                    details[key] = value

        # 5. –§–æ–ª–ª–±–µ–∫: –ø–∞—Ä—Å–∏–º <b>/<strong> –≤–Ω—É—Ç—Ä–∏ –∞–±–∑–∞—Ü–µ–≤, –∫–∞–∫ –≤ –±–ª–æ–∫–µ –æ–ø–∏—Å–∞–Ω–∏—è:
        # <p><b>Available: </b>2,200 ‚Äì 4,752 SF<br><b>Building Size: </b>30,000 SF ...</p>
        for bold in soup.find_all(["b", "strong"]):
            parent = bold.parent
            if not parent:
                continue

            raw_key = bold.get_text(" ", strip=True)
            if not raw_key:
                continue

            key = raw_key.rstrip(":").strip().lower()
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ/–¥–ª–∏–Ω–Ω—ã–µ –∫–ª—é—á–∏
            if not key or len(key) < 3 or len(key) > 60:
                continue

            # –°–æ–±–∏—Ä–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ —Å–æ—Å–µ–¥–µ–π –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ <br> –∏–ª–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ <b>/<strong>
            parts: list[str] = []
            for sib in bold.next_siblings:
                # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ <br> –∏–ª–∏ –Ω–æ–≤–æ–º –∫–ª—é—á–µ
                if hasattr(sib, "name") and sib.name is not None:
                    if sib.name.lower() in ["br", "b", "strong"]:
                        break
                    # –¢–µ–≥–∏ —Å —Ç–µ–∫—Å—Ç–æ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, span)
                    text = sib.get_text(" ", strip=True)
                    if text:
                        parts.append(text)
                else:
                    # –¢–µ–∫—Å—Ç–æ–≤—ã–π —É–∑–µ–ª
                    text = str(sib).strip()
                    if text:
                        parts.append(text)

            value = re.sub(r"\s+", " ", " ".join(parts)).strip()
            if value and key not in details:
                details[key] = value

        return details

    @staticmethod
    def extract_price(soup: BeautifulSoup) -> tuple[str | None, str | None]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–Ω—É –∏ —Ç–∏–ø (sale/lease)
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (price_value, listing_type)
        """
        price_value = None
        listing_type = None
        
        price_pattern = re.compile(r'\$[\d,]+(?:\.[\d]+)?')
        price_elements = soup.find_all(string=price_pattern)
        
        for price_text in price_elements:
            price_str = str(price_text).strip()
            match = price_pattern.search(price_str)
            if match:
                parent = price_text.parent if hasattr(price_text, 'parent') else None
                context = parent.get_text().lower() if parent else price_str.lower()
                
                if any(word in context for word in ['sale', 'for sale', 'asking', 'purchase', 'buy']):
                    price_value = match.group(0)
                    listing_type = 'sale'
                    break
                elif any(word in context for word in ['lease', 'rent', 'rental', 'monthly', 'annual', 'per sqft']):
                    price_value = match.group(0)
                    listing_type = 'lease'
                    break
        
        if not price_value:
            price_divs = soup.find_all(class_=re.compile(r'price|cost|amount', re.I))
            for price_div in price_divs:
                price_text = price_div.get_text(strip=True)
                match = price_pattern.search(price_text)
                if match:
                    price_value = match.group(0)
                    class_name = ' '.join(price_div.get('class', [])).lower()
                    if 'sale' in class_name or 'purchase' in class_name:
                        listing_type = 'sale'
                    elif 'lease' in class_name or 'rent' in class_name:
                        listing_type = 'lease'
                    break
        
        return price_value, listing_type

    @staticmethod
    def extract_listing_type_from_page(soup: BeautifulSoup) -> str | None:
        """
        –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–± –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø —Å–¥–µ–ª–∫–∏ (sale/lease),
        –¥–∞–∂–µ –µ—Å–ª–∏ —Ü–µ–Ω—É –Ω–µ –Ω–∞—à–ª–∏.
        """
        # –ü—Ä–æ–±—É–µ–º –ø–æ —Å–ø–µ—Ü. –±–ª–æ–∫–∞–º
        type_blocks = [
            soup.find(class_=re.compile(r'property_categories_type1_wrapper', re.I)),
            soup.find(class_=re.compile(r'action_tag_wrapper', re.I)),
        ]
        for block in type_blocks:
            if not block:
                continue
            text = block.get_text(" ", strip=True).lower()
            if 'for lease' in text or 'lease' in text or 'for rent' in text:
                return 'lease'
            if 'for sale' in text or 'sale' in text:
                return 'sale'

        # –§–æ–ª–ª–±–µ–∫ –ø–æ –≤—Å–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        page_text = soup.get_text(" ", strip=True).lower()
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π lease, —á—Ç–æ–±—ã –Ω–µ –ø—É—Ç–∞—Ç—å —Å "for sale or lease"
        if 'for lease' in page_text or 'for rent' in page_text:
            return 'lease'
        if 'for sale' in page_text:
            return 'sale'

        return None

    @staticmethod
    def extract_size(soup: BeautifulSoup) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–ª–æ—â–∞–¥—å –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Ñ—É—Ç–∞—Ö"""
        size = None
        
        size_patterns = [
            re.compile(r'([\d,]+(?:\.[\d]+)?)[\s]*sq\.?ft\.?', re.I),
            re.compile(r'([\d,]+(?:\.[\d]+)?)[\s]*square[\s]*feet', re.I),
            re.compile(r'([\d,]+(?:\.[\d]+)?)[\s]*sf\b', re.I),
            re.compile(r'size[:\s]+([\d,]+(?:\.[\d]+)?)', re.I),
        ]
        
        page_text = soup.get_text()
        for pattern in size_patterns:
            match = pattern.search(page_text)
            if match:
                size = match.group(0).strip()
                break
        
        return size

    @staticmethod
    def extract_address(soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–¥—Ä–µ—Å –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)"""
        address = None
        
        # 1. H1 –∑–∞–≥–æ–ª–æ–≤–æ–∫ (—á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –∞–¥—Ä–µ—Å)
        h1 = soup.find('h1')
        if h1:
            h1_text = h1.get_text(strip=True)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ –∞–¥—Ä–µ—Å (—Å–æ–¥–µ—Ä–∂–∏—Ç —Ü–∏—Ñ—Ä—ã –∏ —É–ª–∏—Ü—É)
            if re.search(r'\d+', h1_text) and len(h1_text) > 10:
                address = h1_text
        
        # 2. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã –¥–ª—è –∞–¥—Ä–µ—Å–∞
        if not address:
            address_selectors = [
                soup.find(class_=re.compile(r'address|property-address|location|street', re.I)),
                soup.find(id=re.compile(r'address|property-address|location', re.I)),
                soup.find('div', {'itemprop': 'address'}),
                soup.find('span', {'itemprop': 'address'}),
            ]
            
            for selector in address_selectors:
                if selector:
                    addr_text = selector.get_text(strip=True)
                    if len(addr_text) > 5 and re.search(r'\d+', addr_text):
                        address = addr_text
                        break
        
        # 3. Title —Ç–µ–≥ (–º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∞–¥—Ä–µ—Å)
        if not address:
            title_tag = soup.find('title')
            if title_tag:
                title_text = title_tag.get_text(strip=True)
                # –ò—â–µ–º –∞–¥—Ä–µ—Å –≤ title (–æ–±—ã—á–Ω–æ –≤ –Ω–∞—á–∞–ª–µ –∏–ª–∏ –∫–æ–Ω—Ü–µ)
                if re.search(r'\d+.*(street|st|avenue|ave|road|rd|drive|dr|boulevard|blvd|way|ln)', title_text, re.I):
                    address = title_text.split('|')[0].strip()
        
        # 4. –ú–µ—Ç–∞-—Ç–µ–≥–∏
        if not address:
            meta_address = soup.find('meta', {'property': 'og:street-address'})
            if meta_address and meta_address.get('content'):
                address = meta_address['content']
        
        # 5. –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –∏–ª–∏ –ø–µ—Ä–≤–æ–≥–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
        if not address:
            desc = soup.find(class_=re.compile(r'description|summary|details', re.I))
            if desc:
                desc_text = desc.get_text()
                # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –∞–¥—Ä–µ—Å–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
                addr_match = re.search(r'(\d+\s+[\w\s]+(?:street|st|avenue|ave|road|rd|drive|dr|boulevard|blvd|way|ln)[,\s]+[\w\s]+[,\s]+[A-Z]{2}\s+\d{5})', desc_text, re.I)
                if addr_match:
                    address = addr_match.group(1).strip()
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∞–¥—Ä–µ—Å –∏–ª–∏ fallback –Ω–∞ URL –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏
        return address or "Address not found"

    @staticmethod
    def extract_description(soup: BeautifulSoup) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
        description = None
        
        # 1. –ú–µ—Ç–∞-—Ç–µ–≥ description
        meta_desc = soup.find('meta', {'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            description = meta_desc['content'].strip()
        
        # 2. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –±–ª–æ–∫–∏
        if not description:
            desc_selectors = [
                soup.find(class_=re.compile(r'description|about|details|summary', re.I)),
                soup.find(id=re.compile(r'description|about|details', re.I)),
                soup.find('div', {'itemprop': 'description'}),
            ]
            
            for desc_elem in desc_selectors:
                if desc_elem:
                    description = desc_elem.get_text(strip=True)
                    if len(description) > 50:
                        break
        
        # 3. –ü–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        if not description:
            paragraphs = soup.find_all('p')
            for p in paragraphs:
                text = p.get_text(strip=True)
                if len(text) > 100:
                    description = text
                    break
        
        return description

    @staticmethod
    def extract_listing_status(soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        status = None
        
        status_keywords = {
            'coming soon': ['coming soon', 'coming-soon', 'soon'],
            'available': ['available', 'active', 'for sale', 'for lease', 'on market'],
            'sold': ['sold', 'sale pending'],
            'leased': ['leased', 'rented'],
            'off market': ['off market', 'off-market', 'withdrawn'],
            'pending': ['pending', 'under contract'],
        }
        
        page_text = soup.get_text().lower()
        
        status_elements = soup.find_all(class_=re.compile(r'status|availability|listing-status', re.I))
        for elem in status_elements:
            text = elem.get_text().lower()
            for status_name, keywords in status_keywords.items():
                if any(keyword in text for keyword in keywords):
                    status = status_name
                    break
            if status:
                break
        
        if not status:
            for status_name, keywords in status_keywords.items():
                if any(keyword in page_text for keyword in keywords):
                    status = status_name
                    break
        
        return status or 'available'

    @staticmethod
    def extract_photos(soup: BeautifulSoup, base_url: str) -> list[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–æ—Ç–æ"""
        photos = []
        
        # 1. –ì–∞–ª–µ—Ä–µ—è
        gallery = soup.find(class_=re.compile(r'gallery|slider|carousel|images|photos', re.I))
        if gallery:
            gallery_imgs = gallery.find_all('img')
            for img in gallery_imgs:
                src = img.get('src') or img.get('data-src') or img.get('data-lazy-src') or img.get('data-original')
                if src:
                    full_url = urljoin(base_url, src)
                    if full_url not in photos and 'placeholder' not in full_url.lower():
                        photos.append(full_url)
        
        # 2. –í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if not photos:
            images = soup.find_all('img')
            for img in images:
                src = img.get('src') or img.get('data-src') or img.get('data-lazy-src') or img.get('data-original')
                if src:
                    full_url = urljoin(base_url, src)
                    if (full_url not in photos and 
                        'logo' not in full_url.lower() and 
                        'icon' not in full_url.lower() and
                        'placeholder' not in full_url.lower() and
                        'avatar' not in full_url.lower()):
                        photos.append(full_url)
        
        return photos

    @staticmethod
    def extract_brochure_pdf(soup: BeautifulSoup, base_url: str) -> str | None:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ brochure PDF"""
        brochure_url = None
        
        pdf_links = soup.find_all('a', href=re.compile(r'\.pdf', re.I))
        for link in pdf_links:
            href = link.get('href', '')
            text = link.get_text().lower()
            
            if any(keyword in text for keyword in ['brochure', 'flyer', 'marketing', 'property']):
                brochure_url = urljoin(base_url, href)
                break
        
        if not brochure_url and pdf_links:
            href = pdf_links[0].get('href', '')
            brochure_url = urljoin(base_url, href)
        
        return brochure_url

    @staticmethod
    def extract_agents(soup: BeautifulSoup, base_url: str) -> list[AgentData]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–≥–µ–Ω—Ç–æ–≤ (–æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏ –¥—Ä—É–≥–∏—Ö) –≤ —Å–ø–∏—Å–æ–∫ AgentData.
        –û—Ä–∏–µ–Ω—Ç–∏—Ä—É–µ–º—Å—è –Ω–∞ –±–ª–æ–∫–∏ sidebar + mobile agent area + —Å–µ–∫—Ü–∏—é \"Other Agents\".
        """
        agents: dict[str, AgentData] = {}

        def add_agent(
            name: str | None,
            title: str | None = None,
            photo_url: str | None = None,
            phone: str | None = None,
            link: str | None = None,
        ) -> None:
            if not name:
                return
            key = name.strip().lower()
            if key not in agents:
                agents[key] = AgentData(
                    name=name.strip(),
                    title=title.strip() if title else None,
                    phone_primary=phone.strip() if phone else None,
                    photo_url=photo_url.strip() if photo_url else None,
                    social_media=link.strip() if link else None,
                )
            else:
                agent = agents[key]
                if title and not agent.title:
                    agent.title = title.strip()
                if phone and not agent.phone_primary:
                    agent.phone_primary = phone.strip()
                if photo_url and not agent.photo_url:
                    agent.photo_url = photo_url.strip()
                if link and not agent.social_media:
                    agent.social_media = link.strip()

        # --------- 1. Sidebar –∞–≥–µ–Ω—Ç ---------
        sidebar_unit = soup.find("div", class_=re.compile(r"agent_unit_widget_sidebar_wrapper_unit"))
        if sidebar_unit:
            # –∏–º—è + —Å—Å—ã–ª–∫–∞
            name = None
            link = None
            h4 = sidebar_unit.find("h4")
            if h4:
                a = h4.find("a")
                if a:
                    name = a.get_text(strip=True)
                    link = a.get("href")
                else:
                    name = h4.get_text(strip=True)

            # –¥–æ–ª–∂–Ω–æ—Å—Ç—å
            position_el = sidebar_unit.find(class_=re.compile(r"agent_position"))
            title = position_el.get_text(strip=True) if position_el else None

            # —Ñ–æ—Ç–æ (background-image)
            photo_url = None
            photo_div = sidebar_unit.find(class_=re.compile(r"agent_unit_widget_sidebar"))
            if photo_div:
                style = photo_div.get("style", "")
                # style="background-image: url(https://...jpg)"
                match = re.search(r"url\(['\"]?([^'\")]+)", style)
                if match:
                    photo_url = match.group(1)
                    if not photo_url.startswith("http"):
                        photo_url = urljoin(base_url, photo_url)

            # —Ç–µ–ª–µ—Ñ–æ–Ω –∏–∑ –∫–Ω–æ–ø–∫–∏ Call
            phone = None
            call_link = soup.find("a", class_=re.compile(r"realtor_call"))
            if call_link:
                # —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Ç–µ–∫—Å—Ç –≤–Ω—É—Ç—Ä–∏ <span class="agent_call_no">
                span_phone = call_link.find(class_=re.compile(r"agent_call_no"))
                if span_phone:
                    phone = span_phone.get_text(strip=True)
                else:
                    href = call_link.get("href", "")
                    # href="tel:(508) 651-9017"
                    tel_match = re.search(r"tel:(.+)$", href)
                    if tel_match:
                        phone = tel_match.group(1).strip()

            add_agent(name=name, title=title, photo_url=photo_url, phone=phone, link=link)

        # --------- 2. –ú–æ–±–∏–ª—å–Ω—ã–π –±–ª–æ–∫ –∞–≥–µ–Ω—Ç–∞ ---------
        mobile_blocks = soup.find_all("div", class_=re.compile(r"mobile_agent_area_wrapper"))
        for block in mobile_blocks:
            # –∏–º—è + —Å—Å—ã–ª–∫–∞
            name = None
            link = None
            name_link = block.find("a")
            if name_link:
                name = name_link.get_text(strip=True)
                link = name_link.get("href")

            # —Ñ–æ—Ç–æ
            photo_url = None
            pict_div = block.find("div", class_=re.compile(r"agentpict"))
            if pict_div:
                style = pict_div.get("style", "")
                match = re.search(r"url\(['\"]?([^'\")]+)", style)
                if match:
                    photo_url = match.group(1)
                    if not photo_url.startswith("http"):
                        photo_url = urljoin(base_url, photo_url)

            # —Ç–µ–ª–µ—Ñ–æ–Ω ‚Äì —Ç–æ—Ç –∂–µ, —á—Ç–æ –∏ –≤ sidebar (–µ—Å–ª–∏ –µ—Å—Ç—å)
            phone = None
            call_link = soup.find("a", class_=re.compile(r"realtor_call"))
            if call_link:
                span_phone = call_link.find(class_=re.compile(r"agent_call_no"))
                if span_phone:
                    phone = span_phone.get_text(strip=True)
                else:
                    href = call_link.get("href", "")
                    tel_match = re.search(r"tel:(.+)$", href)
                    if tel_match:
                        phone = tel_match.group(1).strip()

            add_agent(name=name, photo_url=photo_url, phone=phone, link=link)

        # --------- 3. –°–µ–∫—Ü–∏—è \"Other Agents\" (property_other_agents) ---------
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–æ –ø—Ä–∏–º–µ—Ä—É https://rwholmes.com/properties/11-huron-drive-natick/
        other_section = soup.find(id=re.compile(r"property_other_agents", re.I))
        if other_section:
            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ h3/h4 –ø–æ–¥ —ç—Ç–∏–º –±–ª–æ–∫–æ–º ‚Äî —Ç–∞–º –∏–º–µ–Ω–∞ –∞–≥–µ–Ω—Ç–æ–≤
            for heading in other_section.find_all(["h3", "h4"]):
                name = heading.get_text(strip=True)
                if not name:
                    continue
                lower = name.lower()
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∞–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ \"Other Agents\" –∏ –ø–æ—Ö–æ–∂–µ–µ
                if "other agents" in lower:
                    continue

                # –ü–æ–ø—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å –¥–æ–ª–∂–Ω–æ—Å—Ç—å –∫–∞–∫ –±–ª–∏–∂–∞–π—à–∏–π –Ω–µ–ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                title = None
                sib = heading.find_next_sibling()
                while sib is not None and sib.name not in ["h3", "h4"]:
                    text = sib.get_text(" ", strip=True) if hasattr(sib, "get_text") else str(sib).strip()
                    if text:
                        title = text
                        break
                    sib = sib.find_next_sibling()

                add_agent(name=name, title=title, photo_url=None, phone=None, link=None)

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ AgentData
        return list(agents.values())

    def _save_html_if_needed(self, html: str, listing_id: str, url: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç HTML –≤ —Ñ–∞–π–ª, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ (–∫–∞–∂–¥—ã–π N-–π)"""
        self.html_counter += 1
        
        if self.html_counter % self.save_html_every == 0:
            # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ listing_id
            safe_filename = re.sub(r'[^\w\-_\.]', '_', listing_id)
            if not safe_filename or safe_filename == '_':
                safe_filename = f"listing_{self.html_counter}"
            
            filepath = os.path.join(self.html_save_dir, f"{safe_filename}.html")
            
            try:
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(html)
                logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω HTML [{self.html_counter}]: {filepath}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ HTML {filepath}: {e}")

    async def parse_listing(self, url: str) -> DbDTO | None:
        """
        –≠–¢–ê–ü 3: –ü–∞—Ä—Å–∏—Ç HTML –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DbDTO –æ–±—ä–µ–∫—Ç
        """
        # –ü–æ–ª—É—á–∞–µ–º HTML
        html = await self.get_listing_html(url)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'lxml')
        listing_id = self.extract_listing_id_from_url(url) or url
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–π N-–π HTML
        self._save_html_if_needed(html, listing_id, url)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        price, listing_type = self.extract_price(soup)
        if not listing_type:
            # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –ø–æ —Ç–µ–∫—Å—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã (For Lease / For Sale –∏ —Ç.–ø.)
            listing_type = self.extract_listing_type_from_page(soup)
        size = self.extract_size(soup)
        description = self.extract_description(soup)
        listing_status = self.extract_listing_status(soup)
        listing_details = self.extract_details(soup)
        photos = self.extract_photos(soup, self.base_url)
        brochure_pdf = self.extract_brochure_pdf(soup, self.base_url)
        mls_number = self.extract_mls(soup)
        address = self.extract_address(soup)
        agents = self.extract_agents(soup, self.base_url)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º —Ü–µ–Ω—É –Ω–∞ sale_price –∏ lease_price –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
        sale_price = None
        lease_price = None
        if price:
            if listing_type == 'sale':
                sale_price = price
            elif listing_type == 'lease':
                lease_price = price
            else:
                # –ï—Å–ª–∏ —Ç–∏–ø –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω, –ø—ã—Ç–∞–µ–º—Å—è —É–≥–∞–¥–∞—Ç—å –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
                # –ò–ª–∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –æ–±–∞ –ø–æ–ª—è
                sale_price = price
                lease_price = price
        
        # –°–æ–∑–¥–∞–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º DbDTO –æ–±—ä–µ–∫—Ç
        try:
            dto = DbDTO(
                source_name=self.source_name,
                listing_id=listing_id,
                listing_link=url,
                listing_type=listing_type,
                listing_status=listing_status,
                address=address,  # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ
                sale_price=sale_price,
                lease_price=lease_price,
                size=size,
                property_description=description,
                listing_details=listing_details if listing_details else None,
                photos=photos if photos else None,
                brochure_pdf=brochure_pdf,
                mls_number=mls_number,
                agents=agents or None,
                agency_phone=agents[0].phone_primary if agents and agents[0].phone_primary else None,
            )
            return dto
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ DbDTO –¥–ª—è {url}: {e}")
            return None

    # ---------------------- –û–°–ù–û–í–ù–û–ô –ü–†–û–¶–ï–°–° ----------------------

    async def run(self) -> list[DbDTO]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å:
        1. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è - —Å–±–æ—Ä –≤—Å–µ—Ö URL
        2. –ü–æ–ª—É—á–µ–Ω–∏–µ HTML –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ª–∏—Å—Ç–∏–Ω–≥–∞
        3. –ü–∞—Ä—Å–∏–Ω–≥ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ DbDTO –æ–±—ä–µ–∫—Ç–æ–≤
        """
        # –≠–¢–ê–ü 1: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è
        listing_urls = await self.get_listing_urls()
        
        if not listing_urls:
            logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
            return []
        
        # –≠–¢–ê–ü 2 –∏ 3: –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ –ª–∏—Å—Ç–∏–Ω–≥–∞
        results: list[DbDTO] = []
        tasks = [self.parse_listing(url) for url in listing_urls]
        
        logger.info(f"[2-3] –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(listing_urls)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π...")
        
        parsed_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for i, result in enumerate(parsed_results):
            if isinstance(result, Exception):
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {listing_urls[i]}: {result}")
                continue
            
            if result and isinstance(result, DbDTO):
                results.append(result)
                logger.info(f"‚úì [{len(results)}/{len(listing_urls)}] {result.listing_id}")
        
        logger.info(f"\n–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(results)}/{len(listing_urls)}")
        return results


# ---------------------- –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø ----------------------

async def main():
    async with httpx.AsyncClient() as client:
        parser = RwholmesParser(client, concurrency=10)
        results = await parser.run()
        
        if results:
            print("\n–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–≤–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è:")
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º DbDTO –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –≤—ã–≤–æ–¥–∞
            first_dto = results[0]
            print(json.dumps(first_dto.model_dump(), indent=2, ensure_ascii=False))
        
        return results


if __name__ == '__main__':
    asyncio.run(main())
